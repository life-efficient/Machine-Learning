{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Intro\n",
    "\n",
    "As you should know by now, most tasks can be either defined as regression (predicting a continuous value) or classification (predicting a discrete value) problems.\n",
    "\n",
    "The simplest form of a classification problem is __binary classification__:\n",
    "\n",
    "> Special case of classification, where our targets (values to predict) can either take value `0` or `1`.\n",
    "\n",
    "`cat` vs `dog` is an example, where `cat=0` and `dog=1`. That is, every example has a label which is either `True` or `False`.\n",
    "\n",
    "See below for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_binary_data(m=50): \n",
    "    X = np.random.randn(m) #sample from a normal distribution\n",
    "    X = np.array(sorted(X))\n",
    "    Y = X > 0.2    # return binary vector with true where X above some threshold and false if below\n",
    "    return X, Y #returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', marker='x')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "X, Y = make_binary_data()\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, the output of our model could be any real number, negative or positive, unbounded in magnitude. Here the thing is a little more involved.\n",
    "\n",
    "> In classification, we can interpret the model output as a confidence that the example belongs to a particular class.\n",
    "\n",
    "Which bring us to...\n",
    "\n",
    "## Logits\n",
    "\n",
    "> vector (in binary case a scalar) of unnormalized probabilities in the range $(-\\infty, \\infty)$\n",
    "\n",
    "Maybe you've noticed it is __exactly what we've been outputting with regression__!\n",
    "\n",
    "You know what probability is, so logits are pretty similar, but:\n",
    "- __50% chance (probability=$0.5$) is equal to logit=$0$__\n",
    "- Anything below zero is less probable, anything above more\n",
    "\n",
    "Logits are used for classification in order to:\n",
    "- not making unnecessary operation transforming them into probabilities if we are after label (we will see the transformation shortly)\n",
    "- for binary classification every output from our classifier which is greater than `1` is considered `True`, anything below is `False`\n",
    "\n",
    "__Note:__ mathematically, the term logit refers to the log of the odds of a probability and __transforms probability (p below) back into $(-\\infty, \\infty)$ range__.\n",
    "It can be written by the following formula:\n",
    "\n",
    "$$\n",
    "    L = \\ln \\frac{p}{1 - p}\n",
    "$$\n",
    "\n",
    "However, we will use the term logit later to simply refer to the values outputted by the model.\n",
    "\n",
    "## From logits to probabilities\n",
    "\n",
    "We can do this by applying a **sigmoid** function to our output:\n",
    "\n",
    "![](images/sigmoid.jpg)\n",
    "\n",
    "> sigmoid squashes logits from $(-\\infty, \\infty)$ to $(0, 1)$ range which we can easily interpret\n",
    "\n",
    "We can also write a function to compute the derivative of the sigmoid function.\n",
    "We will need this to differentiate our loss with respect to the model parameters, as they only affect the loss through the sigmoid.\n",
    "\n",
    "__Note:__ Sigmoid is the inverse function of logit\n",
    "\n",
    "![](images/sigmoid_deriv.jpg)\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's implement our sigmoid `class` and `sigmoid` function in code. We will code it as and `g.Operation`  as it was done up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aicore.ml.graph as g\n",
    "\n",
    "class _Sigmoid(g.Operation):\n",
    "    def forward(self, inputs):\n",
    "        self.cache = 1 / (np.exp(-inputs) + 1)\n",
    "        return self.cache\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        return upstream_gradient * self.cache * (1 - self.cache)\n",
    "\n",
    "\n",
    "def sigmoid(inputs):\n",
    "    return _Sigmoid()(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we input some values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37754067 0.62245933 0.5       ]\n",
      "[0.04742587 0.73105858 0.73105858]\n",
      "[3.72007598e-44 1.00000000e+00]\n",
      "[1.  0.5]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sigmoid(np.array([-0.5, 0.5, 0])),\n",
    "    sigmoid(np.array([-3, 1, 1])),\n",
    "    sigmoid(np.array([-100, 100])),\n",
    "    sigmoid(np.array([1000000, 0])),\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is one input which destroys our function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-7632d6b793a5>:5: RuntimeWarning: overflow encountered in exp\n",
      "  self.cache = 1 / (np.exp(-inputs) + 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([-1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "What happened, why did we get an error? Value of `np.exp` grows exponentially and `double` (`float64`) type cannot keep such large numbers in memory.\n",
    "\n",
    "Let's check what is the maximum `value` one can pass into `exp(value)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum value: 709.782712893384\n",
      "exp(maximum value): 1.7976931348622732e+308\n",
      "\n",
      "OUTPUT BELOW AS A NON-ML ADDITIONAL CHALLENGE TO SOLVE\n",
      " See Challenges at the end\n",
      "\n",
      "double epsilon: 2.220446049250313e-16\n",
      "exp(maximum value + 255 * epsilon) == exp(maximum value): True\n",
      "exp(maximum value + 255*epsilon): 1.7976931348622732e+308\n",
      "exp(maximum value + 256*epsilon): inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-d164304c9247>:21: RuntimeWarning: overflow encountered in exp\n",
      "  print(f\"exp(maximum value + 256*epsilon): {np.exp(max_value_to_exp + 256*epsilon)}\")\n"
     ]
    }
   ],
   "source": [
    "# Get maximum double value in numpy\n",
    "max_double = np.finfo(\"d\").max\n",
    "max_value_to_exp = np.log(max_double)\n",
    "\n",
    "print(f\"maximum value: {max_value_to_exp}\")\n",
    "print(f\"exp(maximum value): {np.exp(max_value_to_exp)}\")\n",
    "\n",
    "# Get epsilon value for double\n",
    "print(\n",
    "    \"\\nOUTPUT BELOW AS A NON-ML ADDITIONAL CHALLENGE TO SOLVE\\n\",\n",
    "    \"See Challenges at the end\\n\",\n",
    ")\n",
    "epsilon = np.finfo(\"d\").eps\n",
    "print(f\"double epsilon: {epsilon}\")\n",
    "print(\n",
    "    \"exp(maximum value + 255 * epsilon) == exp(maximum value): {}\".format(\n",
    "        np.exp(max_value_to_exp + 255 * epsilon) == np.exp(max_value_to_exp)\n",
    "    )\n",
    ")\n",
    "print(f\"exp(maximum value + 255*epsilon): {np.exp(max_value_to_exp + 255*epsilon)}\")\n",
    "print(f\"exp(maximum value + 256*epsilon): {np.exp(max_value_to_exp + 256*epsilon)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural logarithm $\\ln$ (`np.log` in `numpy`) is inverse of `exp`, hence we can get the maximum this way which is around `709` for double (default) type.\n",
    "\n",
    "__Note:__ `epsilon` is the smallest change which is registered by type. See challenges at the end for more info, this one will be additional (https://en.wikipedia.org/wiki/Octuple-precision_floating-point_format).\n",
    "\n",
    "So, how can we solve the overflow issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable sigmoid\n",
    "\n",
    "`sigmoid` formula can be written as:\n",
    "\n",
    "$$\n",
    "    S(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^{x}}{1 + e^{x}}\n",
    "$$\n",
    "\n",
    "Our previous implementation explodes __only when the value is large (`709`) and negative__. That is because the negative sign is inverted with `-z` and we get positive `709`. Given this:\n",
    "\n",
    "- $e^x$ will overflow when $x$ is positive\n",
    "- $e^{-x}$ will overflow when $x$ is negative\n",
    "\n",
    "This leads us to another exercise...\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Implement stable version of sigmoid. We will use two `helper` functions:\n",
    "- `_negative_sigmoid` - used when values passed as `inputs` are `negative`\n",
    "- `_positive_sigmoid` - used when values passed as `inputs` are `positive`\n",
    "\n",
    "Formulas are provided above, that's the only two functions you have to implement.\n",
    "\n",
    "__Read `sigmoid` code and make sure you understand what is going on and why!__ (you can read [this stackoverflow answer](https://stackoverflow.com/a/64717799/10886420) for in-depth explanation, but that's optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_sigmoid(inputs):\n",
    "    # Second formula for negative values\n",
    "    # Cache exp so you won't have to calculate it twice\n",
    "    exp = np.exp(inputs)\n",
    "    return exp / (exp + 1)\n",
    "\n",
    "\n",
    "def _positive_sigmoid(inputs):\n",
    "    # First formula for positive values\n",
    "    return 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "\n",
    "def sigmoid(inputs):\n",
    "    positive = inputs >= 0\n",
    "    # Boolean array inversion is faster than another comparison\n",
    "    negative = ~positive\n",
    "\n",
    "    # empty contains junk hence will be faster to allocate than zeros\n",
    "    result = np.empty_like(inputs)\n",
    "    result[positive] = _positive_sigmoid(inputs[positive])\n",
    "    result[negative] = _negative_sigmoid(inputs[negative])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version should work correctly for all of the cases below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37754067 0.62245933 0.5       ]\n",
      "[0.04742587 0.73105858 0.73105858]\n",
      "[3.72007598e-44 1.00000000e+00]\n",
      "[1 0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sigmoid(np.array([-0.5, 0.5, 0])),\n",
    "    sigmoid(np.array([-3, 1, 1.])),\n",
    "    sigmoid(np.array([-100., 100.])),\n",
    "    sigmoid(np.array([1000, 0])),\n",
    "    sigmoid(np.array([-1000])),\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss function for binary classification-  Binary cross entropy (BCE)\n",
    "\n",
    "Basic formula for BCE is:\n",
    "\n",
    "$$\n",
    "    BCE = - (y\\ln\\hat{y} + (1-y)\\ln(1-\\hat{y}))\n",
    "$$\n",
    "\n",
    "We can break it down into parts exclusively:\n",
    "- if the label is `1` $(1-y)\\ln(1-\\hat{y})$ is `0`\n",
    "- if the label is `0` $y\\ln\\hat{y}$ is `0`\n",
    "\n",
    "![](./images/bce.jpg)\n",
    "\n",
    "### Why not calculate only one part per label?\n",
    "\n",
    "Can't we calculate one part for `1` label and the second for `0` label with some kind of `if label == 0` switch? Yes, we could, but sometimes in machine learning & deep learning we use technique called `label smoothing`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label smoothing\n",
    "\n",
    "> Label smoothing changes `{0, 1}` labels into \"soft targets\", for example `{0.1, 0.9}`\n",
    "\n",
    "### Why would we do that?\n",
    "\n",
    "- It is impossible to reach `1` value for sigmoid in practice. Hence, even if our predicted probability is `0.99999` we will still have some loss left\n",
    "- Same thing happens for `0` as it is really hard for the classifier to reach exactly this value\n",
    "\n",
    "### How to do that?\n",
    "\n",
    "- there are many implementations. Usually we subtract from largest labels (`1`) some constant and add it to `0` labels (for binary case)\n",
    "- $\\alpha$ is our __smoothing hyperparameter__, usually around `0.1`\n",
    "\n",
    "### How does it help?\n",
    "\n",
    "- Our classifier is __less confident__ about it's predictions. Due to inherit noise in data it is often desirable (especially for the more complex and powerful models, yes, neural networks are prime example)\n",
    "- Predictions are more smooth and gradual. Instead of having rough jumps from `0.999` probability to `0.0001` probability in another case, our algorithms try to distribute the probability more evenly\n",
    "- Classifier __does not try to \"be sure\" about hard examples so much__. As it has to reach `0.9` (let's say) instead of `1`, `0.8` will be also fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy implementation\n",
    "\n",
    "Let's start with naive implementation, simply follow the formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(prediction, label):\n",
    "    return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47407698 0.47407698 0.69314718]\n"
     ]
    }
   ],
   "source": [
    "prediction = sigmoid(np.array([-0.5, 0.5, 0])) \n",
    "labels = np.array([0, 1, 1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as usual, __numerical instability creeps in__. As a rule of thumb, always look suspicious at `exp` and it's inverse function `ln`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([1])\n",
    "labels = np.array([0])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([0])\n",
    "labels = np.array([1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n",
      "<ipython-input-8-f3031382e7cc>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([0])\n",
    "labels = np.array([0])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f3031382e7cc>:2: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n",
      "<ipython-input-8-f3031382e7cc>:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(label * np.log(prediction) + (1 - label) * np.log(1 - prediction))\n"
     ]
    }
   ],
   "source": [
    "prediction = np.array([1])\n",
    "labels = np.array([1])\n",
    "\n",
    "print(bce(prediction, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "- if our model __is very condfident and IS WRONG__ (first two cases) we are left with  `np.log(0)` (which goes towards $-\\infty$, when we take `-` it is simply $\\infty$ we observe). \n",
    "- if our model __is very condfident and IS RIGHT__ (last two cases) we will be left with $ 0 * \\ln{0} $, hence with $ 0 * \\infty $ which is undefined `NaN` (not a number)\n",
    "\n",
    "Can we improve it somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable BCE\n",
    "\n",
    "To stabilize binary cross entropy loss we can mix activation we saw earlier (`sigmoid`) and `binary cross entropy` itself.\n",
    "\n",
    "### Formulation\n",
    "\n",
    "One can derive numerically stable version. It takes some math so a few steps are skipped. You can see the formula that we will use [here](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    BCE & = - (y\\ln\\hat{y} + (1-y)\\ln(1-\\hat{y})) \\\\\n",
    "        & = - (y\\ln(\\frac{1}{1+e^{-x}}) + (1-y)\\ln(1-\\frac{1}{1+e^{-x}}) \\\\\n",
    "        & ... \\\\\n",
    "        & = x - xy + \\ln(1 + e^{-x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Unfortunately this leaves us with $e^{-x}$ once again. After a few math tricks we come to this formula (really, go through it yourself to see some beautiful math tricks):\n",
    "\n",
    "$$\n",
    "    \\max(0, x) - xy + \\ln(1 + e^{-|x|})\n",
    "$$\n",
    "\n",
    "where `x` are logits, `y` are labels\n",
    "\n",
    "You can see no matter the `x` value it will always be negative so `e` can only underflow which is fine for our case (and $\\ln(1 + 0) = 0$). There are also [other formulas](https://discuss.pytorch.org/t/numerical-stability-of-bcewithlogitsloss/8246) but we won't go into details.\n",
    "\n",
    "### Derivative\n",
    "\n",
    "We also have to calculate derivative of `BCEWithLogits` to backpropagate through our model.\n",
    "\n",
    "It is a little cumbersome, but remember you can always use [Wolfram Alpha](https://www.wolframalpha.com/) for such tasks (or it will get you some idea or direction at least).\n",
    "\n",
    "Given that, the derivative is presented as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial BCE}{\\partial x} &= \\sigma(x) - y \\\\\n",
    "    \\frac{\\partial BCE}{\\partial y} &= -x\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "__Implement stable binary cross entropy with logits__\n",
    "\n",
    "- Formula for `forward` and `backward` was already provided\n",
    "- Use `sigmoid` (the stable version) in your implementation of `backward`!\n",
    "- Name the function `bce_with_logits` (and name `class` appropriately according to [`PEP8`](https://www.python.org/dev/peps/pep-0008/) standard)\n",
    "\n",
    "__This binary cross entropy will work directly on `logits`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BCEWithLogits(g.Operation):\n",
    "    def forward(self, logits, targets):\n",
    "        self.cache = (logits, targets)\n",
    "        return (\n",
    "            np.maximum(np.zeros_like(logits), logits)\n",
    "            - logits * targets\n",
    "            + np.log(1 + np.exp(-np.abs(logits)))\n",
    "        )\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        return sigmoid(self.cache[0]) - self.cache[1], -self.cache[0]\n",
    "\n",
    "\n",
    "def bce_with_logits(logits, targets):\n",
    "    return _BCEWithLogits()(logits, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally let's build our binary classifier model class\n",
    "\n",
    "We'll use the chain rule to compute the gradient with our previously defined loss function:\n",
    "\n",
    "![](images/binary_classification.jpg)\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Let's create `BinaryLogisticRegression` class with `sklearn`-like API!\n",
    "\n",
    "For that, we will have to code quite a few methods:\n",
    "- `__init__` - taking `n_features` and `optimizer` (same idea as previously)\n",
    "- `parameters` - returning parameters\n",
    "- `predict_logits(self, X)` - it is essentially linear regression `predict` method\n",
    "- `predict_proba(self, X)` - use `with g.no_grad()` context manager, and return logits  predictions with `sigmoid` function applied on it\n",
    "- `predict(self, X)` - use `with g.no_grad()` context manager and return logits predictions thresholded at `0` value\n",
    "- `fit(self, X, y, epochs: int)` - like previously with linear regression, this time we will use `g.mean` on `bce_with_logits` function (__we are not doing batches here!__)\n",
    "\n",
    "We will make our `BinaryLogisticRegression` with `sklearn` like API and fully stable during `forward` and `backward` pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, n_features, optimizer):\n",
    "        self.W = g.Parameter(np.random.randn(n_features))\n",
    "        self.b = g.Parameter(np.random.randn(1))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def predict_logits(self, X):\n",
    "        return g.add(g.dot(X, self.W), self.b)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        with g.no_grad():\n",
    "            return sigmoid(self.predict_logits(X))\n",
    "\n",
    "    def predict(self, X):\n",
    "        with g.no_grad():\n",
    "            return self.predict_logits(X) > 0\n",
    "\n",
    "    def fit(self, X, y_true, epochs: int = 10):\n",
    "        for _ in range(epochs):\n",
    "            y_pred = self.predict_logits(X)\n",
    "            # loss is our final node\n",
    "            g.mean(bce_with_logits(y_pred, y_true))\n",
    "            g.get().backward()\n",
    "            self.optimizer(self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our binary classifier\n",
    "\n",
    "We will load [breast cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) dataset and split it as per usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, model_selection\n",
    "\n",
    "from aicore.ml import data\n",
    "\n",
    "(X_train, y_train), (X_validation, y_validation), (X_test, y_test) = data.split(\n",
    "    datasets.load_breast_cancer(return_X_y=True)\n",
    ")\n",
    "\n",
    "X_train, X_validation, X_test = data.standardize_multiple(X_train, X_validation, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss before fit: 0.5672309651265559\n",
      "Validation loss before fit: 0.6077853450160681\n",
      "Test loss before fit: 0.6077853450160681\n"
     ]
    }
   ],
   "source": [
    "def calculate_loss(model, X, y_true):\n",
    "    with g.no_grad():\n",
    "        y_pred = model.predict_logits(X)\n",
    "        return g.mean(bce_with_logits(y_pred, y_true))\n",
    "\n",
    "\n",
    "optimizer = g.optimizers.SGD(lr=1e-3)\n",
    "model = BinaryLogisticRegression(n_features=X_train.shape[1], optimizer=optimizer)\n",
    "\n",
    "print(f\"Training loss before fit: {calculate_loss(model, X_train, y_train)}\")\n",
    "print(\n",
    "    f\"Validation loss before fit: {calculate_loss(model, X_validation, y_validation)}\"\n",
    ")\n",
    "print(f\"Test loss before fit: {calculate_loss(model, X_validation, y_validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after fit: 0.03407074401274046\n",
      "Validation loss after fit: 0.0368515028452128\n",
      "Test loss after fit: 0.0368515028452128\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10000)\n",
    "\n",
    "print(f\"Training loss after fit: {calculate_loss(model, X_train, y_train)}\")\n",
    "print(\n",
    "    f\"Validation loss after fit: {calculate_loss(model, X_validation, y_validation)}\"\n",
    ")\n",
    "print(f\"Test loss after fit: {calculate_loss(model, X_validation, y_validation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- What is `log-sum-exp` trick? Check out [this wiki article](https://en.wikipedia.org/wiki/LogSumExp) for more info\n",
    "- Implement non-stable binary cross entropy on your own (which takes __probabilities after sigmoid__ instead of `logits`)\n",
    "- As always, use `sklearn` for this classification task as well\n",
    "- Try implementing function called `binary_smoothing(labels, alpha)` which, given labels, smooths the targets with `alpha` parameter\n",
    "- Use batches, check what is inside `aicore` library in `aicore.ml.data` module\n",
    "- Go around [PEP8](https://www.python.org/dev/peps/pep-0008/) style guide and try to follow conventions provided there from now on (for variable naming, function and class naming at least)\n",
    "- Try to explain the numerical phenomena with epsilon (this one is additional and out of the scope of this course, but you might have some fun)\n",
    "\n",
    "## Summary\n",
    "\n",
    "- the labels for classification problems should be an integer representing the index of the class which that example belongs to\n",
    "- `logits` are unnormalized probability $(-\\infty, \\infty)$\n",
    "- `sigmoid` transforms `logits` into probabilities\n",
    "- naive implementation of `sigmoid` is numerically unstable, more stable version exists\n",
    "- binary cross entropy is a new differentiable loss function that can be optimised to solve classification problems\n",
    "- binary classification can be implemented by having a single boolean integer label for each example, where 1 represents it being a member of that class and 0 represents it not being a member of that class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
