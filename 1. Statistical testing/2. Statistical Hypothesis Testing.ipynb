{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Hypothesis Testing\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Introduction: Statistical Significance & Probability](#Introduction:-Statistical-Significance-&-Probability)\n",
    "- [Normal Distributions, Standardization & Z-tests](#Normal-Distributions,-Standardization-&-Z-tests)\n",
    "- [T-tests](#T\\-tests)\n",
    "- [Chi-squared Tests](#Chi\\-squared-Tests)\n",
    "- [F-tests & ANOVA](#F\\-tests-&-ANOVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Statistical Significance & Probability\n",
    "\n",
    "Having covered the motivation behind statistical hypothesis testing, we will now look at the mechanisms behind this framework so that you can understand how to be able to carry out your own hypothesis tests! We have seen that the aim of hypothesis testing is to help different scientists to reach the same conclusion with the same data. But how to we bring together the ideas of _objectiveness_ and _randomness?_\n",
    "\n",
    "This is where the probability and probability distributions we have encountered come in handy. Since _probability_ is a measure of how likely something is to occur, and _probability distributions_ tell us the probability/probability density of every possible outcome, we can use probability distributions to tell us the __probability of something being true given our null hypothesis__. This means that if the probability of some __test statistic__ and any rarer values being observed, which is based on our data given the null hypothesis, is small _enough,_ we can reject the null hypothesis. This probability is known as a __p-value__, and the _threshold_ of probability for rejecting the null hypothesis is known as the __significance level__ ($\\mathbf{\\alpha}$), which is determined depending on how certain we need to be. If the p-value we compute from our data is lesser than our chosen significance level, our results are __statistically significant__ and we can reject the null hypothesis.\n",
    "\n",
    "__Note:__ The p-value is NOT the probability of observing the test statistic, but the probability of observing the test statistic or any _rarer_ values. Why do you think we don't compute the probability of observing the test statistic?\n",
    "\n",
    "Therefore, we can formalize the steps of hypothesis testing:\n",
    "1. Formulate a hypothesis \n",
    "2. Find the appropriate statistical test\n",
    "3. Choose a significance level\n",
    "4. Collect data and compute test statistic\n",
    "5. Determine the p-value (probability)/critical value\n",
    "6. Reject/accept null hypothesis\n",
    "7. Make a decision\n",
    "\n",
    "We will understand these steps in the context of four widely used statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Distributions, Standardization & Z-tests\n",
    "\n",
    "As we have seen before, the normal distribution is a bell-shaped curve centered around the mean, whose width is determined by its standard deviation. It is a useful distribution, as we can approximate many random variables to have a normal distribution, and the _central limit theorem_ we previously encountered also allows us to make useful approximations. We will now explore the normal distribution in relation to hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to hypothesis testing, we often use the normal distribution to give us measures of probability. However, we tend to simplify the normal distribution to only have the necessary components for testing. So, what parts of the normal distribution actually contribute to the probability we compute? The first thing we need to understand is that the location of the mean is irrelevant. __We only care about the displacement away from the mean__. Secondly, we see from the above plots that the standard deviation affects the probability of a given displacement from the mean, meaning that __the probability depends on the displacement away from the mean proportional to the standard deviation.__\n",
    "\n",
    "This gives us motivation for an incredibly powerful tool, known as __standardization__. The goal of standardization is to compare any normally distributed data to the __standard normal distribution__, which is a normal distribution with a population mean $\\mu = 0$ and a population standard deviation $\\sigma = 1$. This is useful since we don't need the explicit mean and standard deviation, but rather _how many standard deviations away from the mean a value is_ to calculate its probability.\n",
    "\n",
    "To standardize a dataset, we carry out the following two steps:\n",
    "- Subtract the mean from all points in our dataset to ensure it is centered at 0\n",
    "- Divide the result by the standard deviation of the dataset, scaling the results so that the standard deviation is 1\n",
    "\n",
    "For each data value we standardize, we denote it as shown below:\n",
    "\n",
    "$$z = \\frac{x-\\mu}{\\sigma}$$\n",
    "\n",
    "Where $x$ is our data point, referred to as a __raw score__, and $z$ is known as the __z-score__ or __standard score__. By definition, it is a measure of how many standard deviations a data point is from the mean.\n",
    "\n",
    "Why do we bother using the standard normal distribution? Couldn't we get our probabilities from the _raw_ distributions? In practice, you _could_ just use the original normal distribution, even in a statistical test. But by using z-scores, we can use _one_ hollistic distribution to describe the probabilities of _any_ normally distributed datasets. Another reason is that since to get a probability from a pdf, we have to find the area under its curve within a given range, we would to carry out complex operations for each distribution. Instead, statisticians have created a __z-table__, which tells us the _cumulative probability_ of any given z-score. We can use this to compute probabilities for any given normal distribution! While this is not _as_ relevant as when we couldn't use special calculators and computers to give us these probabilities, it still greatly simplifies the process.\n",
    "\n",
    "Let us consider the example that we covered in the Prelude week for the height of students, denoted by $X$, where $X \\sim N(175, 4^{2})$. We saw that for any continuous probability distribution, we can approximate the probability from a PDF with a histogram with very small bin widths. We used it to calculate the probability that $P(X > 180cm)$. Let us have a refresher what this looks like and what the result was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import utils\n",
    "\n",
    "def student_normal(x):\n",
    "    mean, std = 175, 4\n",
    "    exponent = ((x-mean)/std)**2\n",
    "    exponent *= -0.5\n",
    "    output = np.exp(exponent)/(std*np.sqrt(2*np.pi))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utils.prob_estimate(x_lower=180, x_upper=196, pdf_func=student_normal, bin_width=0.001))\n",
    "utils.visualize_normal_interval(interval_floor=180, interval_ceil=196, normal_func=student_normal, mean=175, std=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we use the standard normal distribution to get the same answer?\n",
    "\n",
    "We can standardize our interval, such that:\n",
    "\n",
    "$$P(X>x) \\implies P(Z>\\frac{x-\\mu}{\\sigma})$$\n",
    "$$P(X>180) \\implies P(Z > \\frac{180-175}{4}) = P(Z > 1.25)$$\n",
    "\n",
    "Where 1.25 is the respective _z-score_ or _standard score_ of the raw score of 180cm. We can now apply the same method as we have done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_normal(x):\n",
    "    mean, std = 0, 1\n",
    "    exponent = ((x-mean)/std)**2\n",
    "    exponent *= -0.5\n",
    "    output = np.exp(exponent)/(std*np.sqrt(2*np.pi))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utils.prob_estimate(x_lower=1.25, x_upper=4, pdf_func=standard_normal, bin_width=0.001))\n",
    "utils.visualize_normal_interval(interval_floor=1.25, interval_ceil=4, normal_func=standard_normal, mean=0, std=1, bw=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get the same probabilities in both cases! But how does this work? Well since the factors that affect the probability are the displacement from the mean proportionally to the size of the standard deviation, __the standard normal distribution contains all the necessary information given we have the approapriate z-scores.__ This means for any given interval of any normal distribution, we can always use standardization to look at different normal distributions with the same framework. Another reason why it is useful to have one hollistic distritbution, is that we can look-up __z-tables.__ \n",
    "\n",
    "These look-up tables contain the __cumulative probability__ of the standard normal distribution for any given _z-score_ ($\\Phi(z)$). The cumulative probability tells us the probability $P(-\\infty < Z < z)$ = $P(Z < z)$, and using either one or multiple cumulative probabilities from our z-table, we can calculate the probability of any interval from any normal dsitribution! Below we show what cumulative probabilities mean with a visualization for different z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_list = [-3, 0, 1.25, 3, 4]\n",
    "\n",
    "for z in z_list:\n",
    "    prob = utils.prob_estimate(x_lower=-4, x_upper=z, pdf_func=standard_normal, bin_width=0.001)\n",
    "    print(\"cumulative_probability(z={z_val}) = {probability}\".format(z_val=z, probability=prob))\n",
    "    utils.visualize_normal_interval(interval_floor=-4, interval_ceil=z, normal_func=standard_normal, mean=0, std=1, bw=0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm these results with the convential z-tables available online, such as [here](http://www.z-table.com/). Now we can see a third method for finding the probability $P(X>180)$. The method is as follows:\n",
    "\n",
    "$$P(X>180) = P(Z>1.25) $$\n",
    "$$ = 1 - P(Z<1.25) $$\n",
    "$$ = 1 - \\Phi(1.25) $$\n",
    "$$ = 1 - 0.8944 = 0.1056$$\n",
    "\n",
    "Which gives us the same probability once again. Using this understanding of the standard normal distribution, we can carry out what are known as __z-tests__. This test uses the assumption of normality for the underlying data to determine whether the hypothesis you have chosen has a probability (p-value) greater than the significance level ($\\alpha$) we have chosen. We can either approximate the distribution of different continuous random variables to normal distributions (given they meet the appropriate criteria) or apply the _central limit theorem (CLT)_ to compare the means of different datasets. \n",
    "\n",
    "Let us consider this example: Netflix wants to officially introduce a new feature and wants to verify that it will improve user-experience. Their method for quantifying the quality in user experience is average daily streaming time for their user base. Therefore, by looking at a given user's average daily streaming time, Netflix can set up a __control group__, with users without using the new feature, and an __experiment group__, with users using the new feature. \n",
    "\n",
    "Since Netflix has access to every user's average daily stream time, they have determined the population mean of the control group to be $\\mu_{0} = 1.55$ hours. On the other hand, to determine the effectiveness of the new feature, Netflix carries out an experiment with 100 users, all being introduced to the new feature, and collecting their average daily stream time. If we can show that the mean of the population with a new feature is __significantly__ greater than the population mean of the control group, we have shown it to be have a __significant__ improvement on user experience. Let us now carry out a statistical hypothesis test to make a decision based on the data collected, which we import as a dataframe below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "stream_data = pd.read_csv(\"../DATA/stream_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrying out the steps we have above, we can solve this problem.\n",
    "\n",
    "### 1. Formulate a hypothesis\n",
    "Given our 'innocent before proven guilty' mentality, we can define our null and alternative hypotheses as follows:\n",
    "\n",
    "$\\mathbf{H_{0}}$: The new feature has no effect on average daily stream time\n",
    "\n",
    "$\\mathbf{H_{a}}$: The new feature increases average daily stream time\n",
    "\n",
    "### 2. Find the appropriate statistical test\n",
    "Since, according to the CLT, the sample mean of our experiment group has approximately a normal distribution (given that $n=100$) and the population variance can be approximated by the sample variance since $n$ is large (law of large numbers), we can carry out a _z-test_.\n",
    "\n",
    "### 3. Choose a significance level\n",
    "Given that the feature is not difficult to implement and can significantly boost capital, we do not need to have a strict significance level. Therefore, we will use a significance level of $\\alpha=0.05$.\n",
    "\n",
    "### 4. Collect data and compute test statistic\n",
    "As we have already collected our data, we need to compute our test statistic, which for a z-test is know as the __z-statistic__. Given we assume our null hypothesis to be true, we treat the sample mean as a sample from the normal distribution given by the CLT with $\\mu_{0}$ as the population mean. If there is a very small probability of this being the case, we can reject our null hypothesis. To compute this, we will also estimate the sample standard deviation. We will use Python to carry out the following process:\n",
    "\n",
    "$$z = \\frac{\\bar{x} - \\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}} \\approx \\frac{\\bar{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}} $$\n",
    "\n",
    "__Note__: Given the CLT, we are assuming that the sample mean of our data follows a normal distribution with a population mean of $\\mu_{0}$ and a variance equal to the ratio of our population variance to the sample size $\\frac{\\sigma^{2}}{n}$. Given a large enough sample size, we can safely approximate this population standard deviation given the sample standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Computing test statistic\n",
    "mu_0 = 1.55\n",
    "stream_time = stream_data[\"Average Stream Time (h)\"]\n",
    "\n",
    "sample_std = np.std(stream_time)\n",
    "sample_mean = np.mean(stream_time)\n",
    "n = stream_time.shape[0]\n",
    "\n",
    "z_statistic = (sample_mean - mu_0)/(sample_std/(np.sqrt(n)))\n",
    "print(\"z-statistic:\", z_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Determine the p-value (probability)/critical value\n",
    "We can use our _prob\\_estimate(  )_ function, an online z-table, or just use a Python library that contains the z-table. The z-table gives us $p = 1 - 0.98 = 0.02$. We show the other two methods below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Using our own prob_estimate function that uses histogram approximations\n",
    "p1 = utils.prob_estimate(x_lower=z_statistic, x_upper=4, pdf_func=standard_normal, bin_width=0.001)\n",
    "print(\"p1:\", p1)\n",
    "\n",
    "# Using scipy.stats\n",
    "p2 = stats.norm.sf(z_statistic)\n",
    "print(\"p2:\", p2)\n",
    "\n",
    "# Plotting the interval\n",
    "utils.visualize_normal_interval(interval_floor=z_statistic, interval_ceil=4, normal_func=standard_normal, mean=0, std=1, bw=0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reject/accept null hypothesis\n",
    "Given that our p-value (0.02), which is the probability of our sample mean or anything rarer being observed given a normal distribution centered at $\\mu_{0}$, is much smaller than our significance level, $\\alpha$ (0.05), it allows us to reject the null hypothesis, meaning that the sample mean of average daily stream time for users of the new feature is __significantly__ larger than the population mean of users without the new feature.\n",
    "\n",
    "The interval with the orange histogram is known as the __rejection region__ of this hypothesis test, and has a probability equal to the significance level. If our test-statistic falls within this region, we may reject the null hypothesis. The test statistic closest to the mean in the rejection region is known as the __critical value.__ In this example, we are carrying out a __one-tailed test__, as we only consider an effect in one direction. The critical value for a one-tailed test with $\\alpha=0.05$ can be found in a z-table, which is $z_{0.05} = 1.65$. Since our z-statistic is much larger than this, it is within the rejection region and we can reject our null hypothesis. This is an alternative approach for rejecting/not rejecting the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make a decision\n",
    "Since the feature has a statistically significant impact on average usage of the product, we have used our data to objectively suggest that the new feature should be implemented!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have seen is known as a __one sample z-test__, which determines whether the sample mean is statistically different from a known or hypothesized population mean. We can also use this framework in what are known as __two-sample z-tests__ to determine whether the average difference between two different populations are significantly different, or instead due to random chance. Because of the CLT, is it is also useful to use the z-statistic for __two proportion s z-test__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-tests\n",
    "We can now travel back in time to learn about the __Student's t-test.__ In the past, z-tests used to be commonplace, but were only realiable for a large enough sample size (generally $n>30$). However, when the sample size was small, the sample variance was not a good enough approximation of the population variance, hence came William Gosset. He was a statistician and brewer that worked for Guiness. His role was to study and test which ingredients would ensure the optimal optimal brew. To overcome the limitations of the z-test for means of small sample sizes, he devised the __t-test__. In order for Guiness to keep their competitive advantage, Gosset was not allowed to openly publish his results. Therefore, he published his results under the pseudoname of Student. This is the reason why, even today, the test is referred to as Student's t-test!\n",
    "\n",
    "<img src=\"../images/gosset.jpg\" width=\"200px\" height=\"100px\"/>\n",
    "\n",
    "So, why do we need the t-test and how does it work? As previously mentioned, the z-test is reliable as long as either the population variance is known (which highly unlikely) or we are able to estimate it from the sample variance, given that the number of observations is large enough. Let us look closer at how this works.\n",
    "\n",
    "Imagine we are sampling from a continuous random variable that follows an exponential distribution with population mean $\\mu=3$ and population standard deviation $\\sigma=3$. Let us compute the sample standard deviation for different sample sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals = [2, 15, 50, 200] # different sample sizes to test\n",
    "\n",
    "for n in n_vals:\n",
    "    std_list = []\n",
    "    for i in range(5):\n",
    "        x = np.random.exponential(scale=3, size=n) # n observations\n",
    "        std_list.append(np.std(x))\n",
    "    print(\"n:\", n, \"sample standard deviations:\", std_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that, according to the law of large numbers, our approximation of the population standard deviation improves as our number of observations increases. This is further emphasised by the fact that when we calculate the z-statistic, the standard deviation of the distribution is given by the quantity $\\frac{s}{\\sqrt{n}}$. If we find this quantity below for the same cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_vals:\n",
    "    std_list = []\n",
    "    for i in range(5):\n",
    "        x = np.random.exponential(scale=3, size=n) # n observations\n",
    "        std_list.append(np.std(x)/np.sqrt(n))\n",
    "    print(\"n:\", n, \"scaled stds:\", std_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the larger the number of observations, the more appropriate it is to replace the population standard deviation with the sample standard deviation. But what if we do not have access to that many observations? This means that the sample standard deviation can no longer approximate the population standard deviation, and is now itself a random variable. This is clear from the fact that every time we calculated the standard deviation above, we got a different value. Therefore, __we do not know all the required parameters to use the normal distribution for statistical testing.__\n",
    "\n",
    "Therefore, for small sample sizes (generally $n<30$), rather than computing a z-statistic, we compute a __t-statistic__ to carry out a __t-test__ for independent identically distributed samples. We generally assume the underlying distribution to be normal for this test to work, while according to William Gosset himself: \"it appears probable that the deviation from normality must be very extreme to load to serious error\". This statistic given by the following equation.\n",
    "\n",
    "$$t_{n-1} = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}}$$\n",
    "\n",
    "Which follows a __t-distrbution__ with the same parameters as the corresponding standard normal distribution, but with the additional parameter of __degrees of freedom (df)__, where $df = n-1$. Degrees of freedom in statistics refers to the number of values in the final calculation of a statistic that are free to vary. Let us see what the t-distributions look like with respect to the corresponding normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "dfs = [3, 10, 50]\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Plotting different t-distributions\n",
    "fig = go.Figure()\n",
    "for df in dfs:\n",
    "    rv = stats.t(df)\n",
    "    fig.add_trace(go.Scatter(x=x, y=rv.pdf(x), name=\"t, df = {df}\".format(df=df)))\n",
    "fig.add_trace(go.Scatter(x=x, y=standard_normal(x), name=\"standard normal\", line=dict(dash=\"dash\")))\n",
    "fig.update_layout(\n",
    "    title=\"T-distributions\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"f(x)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the plot that the t-distribution is a wider bell-shaped curve than the standard normal distribution, where the 'degrees of freedom' parameter determines the spread of the PDF. This makes sense, since a lower $df$ implies more uncertainty, the distribution becomes wider. Hence, as we have more and more observations, the t-distribution becomes narrower, approaching the the standard normal distribution. Therefore, even if we used a t-statistic and t-test when we have a large number of samples, we would get the same p-values as from a z-test! \n",
    "\n",
    "For example, if we used the t-test for the Netflix scenario, we  would get the t-statistic $t=2.05$, and have $df=100-1 = 99$. We will not cover the PDF of t-distributions in the scope of this course, but to calculate the correct p-value, we can also either use a t-table available [online](http://www.ttable.org/) (which works the same as a z-table), or an in-built Python program. From the t-table, we get a p-value $p=0.02$, just as we saw before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-value from t-distribution\n",
    "t_statistic = 2.05\n",
    "df = 100-1\n",
    "p = stats.t.sf(abs(t_statistic), df=df)\n",
    "print(\"p:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now carry out an example. Let us consider Amazon delivery standards, where the more accurate the delivery time, the more reliability they can relay to their customers. For commercial purposes, they would like to make a public annoucement that their delivery time is, on average, 30 minutes offset from their predicted time. To prevent misinformation, you are hired as an impartial data scientist to run a quick program to gather data and try to prove the opposite. We are able to collect a sample of $n=17$ observations of offset delivery time in minutes. Assuming normality, given this sample, we will use a statistical t-test to determine whether we can say whether the true offset delivery time is significantly different from 30 minutes or not.\n",
    "\n",
    "Before we begin, it is import to note the difference between this type of statistical test:\n",
    "\n",
    "- While before we used a __one-tailed hypothesis test__, which accounts for deviations along only one direction, this test uses a __two-tailed hypothesis test__, which accounts for deviations in both directions\n",
    "\n",
    "To account for deviations in both directions, the probability corresponding to our chosen significance level, $\\alpha$, is split in half between both sides, as shown below for the case of an $\\alpha=0.05$ in the diagram below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_two_tail(critical_value=1.96, normal_func=standard_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the diagram, it can be seen that from the 0.05 probability of rejecting the null hypothesis, 0.025 of that probability is distributed on one side of the distribution, and the other 0.025 of the probability on the other side. As we are testing whether Amazon's true offset time shows a significant difference from their claimed time regardless of direction, we will need to use a two-tailed test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_data = pd.read_csv(\"../DATA/amazon_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Formulate a hypothesis\n",
    "$\\mathbf{H_{0}}$: The average offset delivery time is 30 minutes\n",
    "\n",
    "$\\mathbf{H_{a}}$: The average offset delivery time is not 30 minutes\n",
    "\n",
    "### 2. Find the appropriate statistical test\n",
    "Given that the population standard deviation is not know, we can assume the data is approximately normally distribution and we only have $n=17$ observations, we will use the two-tailed t-test.\n",
    "\n",
    "### 3. Choose a significance level\n",
    "Given that Amazon is remakably confident of their delivery time accuracy, they want to carry out a process of verification that is strict enough to easily reject the null hypothesis, choosing a siginificance level, $\\alpha=0.1$.\n",
    "\n",
    "### 4. Collect data and compute test statistic\n",
    "As we have already collected our data, we need to compute our test statistic, which for a t-test is know as the __t-statistic__. We will use Python to carry out the following process:\n",
    "\n",
    "$$t = \\frac{\\bar{x} - \\mu_{0}}{\\frac{s}{\\sqrt{n}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing test statistic\n",
    "mu_0 = 30\n",
    "offset_time = offset_data[\"0\"]\n",
    "\n",
    "sample_std = np.std(offset_time)\n",
    "sample_mean = np.mean(offset_time)\n",
    "n = offset_time.shape[0]\n",
    "\n",
    "t_statistic = (sample_mean - mu_0)/(sample_std/(np.sqrt(n)))\n",
    "print(\"t-statistic:\", t_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Determine the p-value (probability)/critical value\n",
    "Using an online t-table, or just use a Python library that contains the t-table. The t-table for a two-tailed t-test with $\\alpha=0.1$ for $df=17-1=16$ gives us a critical value of $t_{crit} = 1.746$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reject/accept null hypothesis\n",
    "Given that $t_{statistic} < t_{crit} $ for the chosen significance level, $\\alpha=0.1$, we cannot reject the null hypothesis, meaning that the Amazon predicted average offset delivery time is not significantly different from the population mean offset delivery time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make a decision\n",
    "Given for our statistical test, we were not able to find a significant difference between the predicted offset delivery time and true offset delivery time, Amazon can make their claim on delivery time accuracy to boost their credibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with a z-test, you can also carry out a __two-sample (independent) t-test__ for comparing the means of two different groups, or a __paired t-test__ for comparing the mean of the same group at different times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-squared Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __chi-square test__ is a statistical test that uses a __chi-square ($\\mathbf{\\chi^{2}}$) distribution__ and we compute instead a __chi-square statistic.__ While still requiring the assumpting of normality, it is used in a different context from the z-tests and t-tests. It is incredibly powerful as it allows us to analyse the relationship between two categorical quantities. \n",
    "\n",
    "By definition, it is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more __mutually exclusive__ categories of a __contingency table.__ A contingency table is a matrix that stores the frequency distribution of variables. So how does the chi-square test work? First, let us look at a toy machine learning example with the input feature 'sex' and output 'handedness', referring to a person's biological sex and dominant hand respectively. We want to determine whether the output of a machine learning model, 'handedness', is dependent on the 'sex' category. If independent, it may be a feature we can remove from our dataset.\n",
    "\n",
    "|        | Right handed | Left handed | Total |\n",
    "|--------|--------------|-------------|-------|\n",
    "| Male   | 43           | 9           | 52    |\n",
    "| Female | 44           | 4           | 48    |\n",
    "| Total  | 87           | 13          | 100   |\n",
    "\n",
    "From this data, how can we infer any information regarding the relationship between 'sex' and 'handedness'? Well, if these variables have no significant relationship, then we would expect the same frequency ratio of males to females in the right handed group and left handed group. With this assumption, we can calculate the expected frequecy for every __cell__ of our contingency table. There comes the key idea of the chi-square test: __if the observed values vary too much from the expected values if there was no relationship, there must be some relationship between the two variables.__ Therefore, we can calculate the chi-square statistic as follows:\n",
    "\n",
    "$$\\chi^{2} = \\sum_{i=1}^{n}\\frac{(O_{i} - E_{i})^{2}}{E_{i}} $$\n",
    "\n",
    "By summing the square of the differences and scaling by the size of the expected values of each cell, we get the overall deviation from the case which we would expect if there was no relationship. Now comes the question: how can we quantify this difference in the framework we have already seen with hypothesis tests? What probability distribution does this statistic follow? Here is where the magic of statistics happens. If we can assume that the frequencies are themselves __normally distributed__, then the chi square is the sum of the square of $n$ normally distributed random variables. In this case, the statistic will follow a chi square distribution governed by the number of cells, or the number of rows and columns in our contingency table.\n",
    "\n",
    "Let us compute the $\\mathbf{\\chi^{2}}$ statistic for this example. We will start by determining the expected value of each cell if the two variables are independent. If $\\frac{52}{100} = 0.52$ of the sample are males, we expect 52% of right handed subjects to be male. Therefore, we can compute the expected vaue as follows:\n",
    "\n",
    "$$E_{RH-M} = 87 \\times \\frac{52}{100} = 45.24$$\n",
    "\n",
    "The same logic applies for the expected value of other cells, as shown below:\n",
    "$$E_{RH-F} = 87 \\times \\frac{48}{100} = 41.76$$\n",
    "$$E_{LH-M} = 13 \\times \\frac{52}{100} = 6.76$$\n",
    "$$E_{LH-F} = 13 \\times \\frac{48}{100} = 6.24$$\n",
    "\n",
    "And now that we have the expected values of each cell, we can compute our statistic:\n",
    "\n",
    "$$\\chi^{2} = \\sum_{i=1}^{n}\\frac{(O_{i} - E_{i})^{2}}{E_{i}} = \\frac{(43 - 45.24)^2}{45.24} + \\frac{(44 - 41.76)^2}{41.76} + \\frac{(9-6.76)^{2}}{6.76} + \\frac{(4-6.24)^{2}}{6.24} = 1.777$$\n",
    "\n",
    "This statistic now follows a chi-square distribution! To determine the exact distribution, we need to find the final additional parameter, which is the degrees of freedom. A common misconception is to assume that the number of degrees of freedom is equal to the number of cells, but that is not the case. Because the calculation uses the row/column totals, the last cell of each row/column is NOT free to vary. Therefore, the degrees of freedom can be calculated as follows:\n",
    "\n",
    "$$df = (\\text{number of rows} - 1)(\\text{number of columns} - 1)$$\n",
    "\n",
    "Which in the case of our example is simply $df=1$. Using our statistic and $df$, we can use a __chi-square table__, available [here](https://www.medcalc.org/manual/chi-square-table.php) or a Python library that includes the chi-square distribution. Below we plot how the chi-square distribution changes with respect to degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting chi-square distributions\n",
    "dfs = [1, 2, 5, 10]\n",
    "x_vals = np.linspace(0.5, 8, 1000)\n",
    "\n",
    "fig = go.Figure()\n",
    "for df in dfs:\n",
    "    fig.add_trace(go.Scatter(x=x_vals, y=stats.chi2.pdf(x_vals, df), name=\"df = \" + str(df)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Chi-square distributions\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"f(x)\"\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distributions above we can see that as the number of $df$ changes, the distribution changes accordingly, as it implies we are adding together more and more squares of normally distributed random variables. We will now consider this example in the hypothesis testing framework that we have previously encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Formulate a hypothesis\n",
    "As previously mentioned, we assume first that the two categorical variables are independent:\n",
    "\n",
    "$\\mathbf{H_{0}}$: The two features are independent\n",
    "\n",
    "$\\mathbf{H_{a}}$: The two features are not independent\n",
    "\n",
    "### 2. Find the appropriate statistical test\n",
    "As we are dealing categorical variables, we will use the chi-square test to determine whether there is a relationship between the two variables or whether the differences from the expected are due to chance.\n",
    "\n",
    "### 3. Choose a significance level\n",
    "For this intended purpose we will continue with a significance level of $\\alpha=0.05$.\n",
    "\n",
    "### 4. Collect data and compute test statistic\n",
    "We have already computed the chi-square statistic, but below we also show how it can be computed using Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing chi-square statistic using Python\n",
    "obs = np.array([[43, 9], [44, 4]])\n",
    "chi2, p_val,_,_ = stats.chi2_contingency(obs, correction=False)\n",
    "print(\"chi-square:\", chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Determine the p-value (probability)/critical value\n",
    "Given the statistic computed and the known $df$, we can use a chi-square table to determine the critical value, which for our significance level of $\\alpha=0.05$ is $\\chi^{2}_{C}=3.84$. We can also simply use a Python library to compute the corresponding p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already computed p-value from previous cell\n",
    "print(\"p-value:\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reject/accept null hypothesis\n",
    "Given that our p-value (0.182), which is the probability of our $\\chi^{2}$ statistic being observed given it follows a $\\chi^{2}$-distribution, is much larger than our significance level, $\\alpha=0.05$, we cannot reject the null hypothesis. The same conclusion can be reached by observing that our computed statistic is lesser than the critical value observed in the chi-square table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make a decision\n",
    "Our results show that 'handness' shows no significant dependence on 'sex'. Thus, we can remove the 'sex' feature from our machine learning model without a significant impact on its accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have just carried out is known as a __chi square test of independence__, which as the name implies, is use to investigate the dependence of one categorical variable on another. We can also use __chi-square goodness of fit tests__ to evaluate how well the observed distribution of our data meets some expected distribution for all the possible groups of a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-tests & ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some of you may have already guessed given the context of this topic, __F tests__ are statistical tests that use __F statistics__, which are statistics that follow a __F distribution__. Most commonly, F-tests are useful when testing whether the mean of a set of __normally distributed populations__, all having the same standard deviation, are equal. This plays a vital role in the __analysis of variance (ANOVA),__ which are a group of models and procedures used to analyze the difference between group means in a sample. This procedure can be seen as a statistical test that generalizes the t-test beyond two means. We will analyse this relationship by going over a __one-way ANOVA,__ also known as a __single-factor ANOVA.__\n",
    "\n",
    "This form of ANOVA is used when you have a categorical independent variable (with two or more categories) and a normally distributed interval (continuous) dependent variable and you wish to __test for differences in the means of the dependent variable broken down by the different categories of the independent variable.__ In this context, the F-statistic is calculated as follows:\n",
    "\n",
    "$$F = \\frac{\\text{explained variance}}{\\text{unexplained variance}} = \\frac{\\text{between-group variability}}{\\text{within-group variability}}$$\n",
    "\n",
    "How does this statistic help us quantify the differences in group means for different groups? Let us consider a simple toy example of whether dog breed (categorical independent variable) affects dog height (interval dependent variable). Let us say we collected data on three different dog breeds: Alaskan Klee Kais, Siberian Huskies and Alaskan Malamutes. If we collected the heights of 20 dogs per breed. \n",
    "\n",
    "From this data, what do we need to account for to determine whether the mean of dog height varies between different breeds? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<table><tr><td><img src='https://mydogsinfo.com/wp-content/uploads/2020/05/alaskan-klee-kai-scaled-e1579557972696.jpg'></td><td><img src='https://petcentral.chewy.com/wp-content/uploads/siberian-husky-woods-shutterstock_558432511.jpg'></td><td><img src='https://www.rover.com/blog/wp-content/uploads/2019/11/top-alaskan-malamute-dog-names-960x540.jpg'></td></tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sample provides us with an estimate of the height distribution of each breed. How can we determine whether the centre of these distributions (population mean for each dog breed) are significantly different? The first factor to consider is height difference between groups. The __larger__ the height difference on average (variance) for different breeds (groups), the __more likelihood__ there is of the population means being different. We must also account for difference in heights between breeds (groups). The __larger__ the average height difference (variance) within each breed, the __less likelihood__ there is of there being a significant difference in population mean between different breeds (groups). This becomes even clearer by plotting different cases of height distributions for each breed (assuming the continuous variable to be normally distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Variance Visualization\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "cases = (\"High between, high within\", \"High between, low within\", \"Low between, high within\", \"Low between variance, low within\")\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    subplot_titles=cases)\n",
    "\n",
    "mean_sets = [[40, 70, 100], [50, 60, 70]]\n",
    "variances = [20, 5]\n",
    "n_case = 0\n",
    "x_vals = np.linspace(20, 120, 1000)\n",
    "names = [\"klee kai\", \"husky\", \"malamute\"]\n",
    "colors = [\"orange\", \"black\", \"red\"]\n",
    "\n",
    "for idx1, mean_set in enumerate(mean_sets):\n",
    "    for idx2, variance in enumerate(variances):\n",
    "        for color, dog_name, mean in zip(colors, names, mean_set):\n",
    "            current_pdf = stats.norm.pdf(x_vals, mean, variance**0.5)\n",
    "            if idx1+idx2==0:\n",
    "                fig.add_trace(go.Scatter(x=x_vals, y=current_pdf, marker_color=color, name=dog_name), row=idx1+1, col=idx2+1)\n",
    "            else:\n",
    "                fig.add_trace(go.Scatter(x=x_vals, y=current_pdf, marker_color=color, showlegend=False), row=idx1+1, col=idx2+1)\n",
    "fig.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more the distributions overlap, the harder to distinguish whether the population means of each group are different, as shown above. So how do we calculate the F-statistic?\n",
    "\n",
    "Let us assume that our categorical independent variable has __I__ populations (groups) and each population has __J__ observations. First, we need to calculate 3 intermediary quantities:\n",
    "\n",
    "$$\\text{The sample mean of the }i^{th}\\text{ population: }\\bar{X}_{i} = \\frac{1}{J}\\sum_{j=1}^{J}X_{ij} $$\n",
    "$$\\text{The sample variance of the }i^{th}\\text{ population: }S_{i}^{2} = \\frac{1}{J-1}\\sum_{j=1}^{J}(X_{ij} - \\bar{X_{i}})^{2}$$\n",
    "$$\\text{The grand sample mean: }\\bar{X} = \\frac{1}{I}\\sum_{i=1}^{I}\\bar{X_{i}} $$\n",
    "\n",
    "With these defined quantities, we can now calculate the f-statistic. As ANOVA is common place when analysing of the effectiveness on medical treatments on patients, we can calculate the __mean square of treatments (MSTr):__\n",
    "\n",
    "$$MSTr = \\frac{J}{I-1}\\sum_{i=1}^{I}(\\bar{X_{i}} - \\bar{X})^{2}$$\n",
    "\n",
    "As well as the __mean square error__:\n",
    "\n",
    "$$MSE = \\frac{1}{I}\\sum_{i=1}^{I}(S_{i}^{2})$$\n",
    "\n",
    "And for the final step, we can calculate the F-statistic, which can be expressed in two forms:\n",
    "\n",
    "$$F = \\frac{MSTr}{MSE} = \\frac{SS_{Treatments}/(I-1)}{SS_{Error}/I(J-1)}$$\n",
    "\n",
    "While the second form of the F-statistic is not necessarily useful in calculations, it shows that the denominator and numerator of the statistic are given by the sum of the respective square errors, which follow a chi-square distribution, divided by their respective degrees of freedom. In other words, the F-statistic follows a distribution given by the ratio of two scaled chi-square distributed variables! Therefore, this F-distribution has two parameters, which are the __numerator degrees of freedom__ ($\\mathbf{\\nu_{1}}$) and __denominator degrees of freedom__ ($\\mathbf{\\nu_{2}}$). As we saw before, the degrees of freedom of the sample variance of a sample with $N$ samples is $N-1$, as the use of the sample mean removes one degree of freedom from statistic. With the same logic, we can see that the MSTr must have I-1 degrees of freedom. For calculating the MSE, we have I$\\times$J observations, and use 'I' group sample means for the calculation. Therefore, our degrees of freedom are given as follows:\n",
    "\n",
    "$$\\nu_{1} = I-1$$\n",
    "$$\\nu_{2} = JI - I = I(J-1)$$\n",
    "\n",
    "Along the lines of our dog example, if we have three groups/populations ($I=3$) and each group has 20 observations ($J=20$), our degrees of freedom can be calculated to be $\\nu_{1} = 3-1 = 2$ and $\\nu_{2} = 20(3-1) = 40$. Below we show some examples of F-distributions for different combinations of degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f-distribution visualization\n",
    "\n",
    "v1_list = [2, 2, 5, 5]\n",
    "v2_list = [20, 40, 20, 40]\n",
    "x_vals = np.linspace(0, 5, 1000)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for v1, v2 in zip(v1_list, v2_list):\n",
    "    current_pdf = stats.f.pdf(x_vals, v1, v2)\n",
    "    fig.add_trace(go.Scatter(x=x_vals, y=current_pdf, name=\"v1={v1}, v2={v2}\".format(v1=v1, v2=v2)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"F-distributions\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"f(x)\"\n",
    "    )\n",
    "fig.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us carry out an one-way ANOVA F-test to determine whether the population mean height of the three different dog breeds show significant differences given our sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Formulate a hypothesis\n",
    "\n",
    "$\\mathbf{H_{0}}$: The population mean height is equal for each dog breed\n",
    "\n",
    "$\\mathbf{H_{a}}$: The population mean height is significantly different for one or more dog breeds\n",
    "\n",
    "### 2. Find the appropriate statistical test\n",
    "As mentioned, when dealing with a single categorical independent variable (dog breed) and an ratio dependent variable, we can use the one-way ANOVA.\n",
    "\n",
    "### 3. Choose a significance level\n",
    "Given that we are doing this out of curiosity, we will not be as meticulous as previously and choose a significance level of $\\alpha=0.10$.\n",
    "\n",
    "### 4. Collect data and compute test statistic\n",
    "Below, we read-in our dog height data and compute the f-statistic by applying the equations above in Python as well as using the _scipy.stats_ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in our data\n",
    "dog_data = pd.read_csv(\"../DATA/dog_data.csv\")\n",
    "dog_data = np.array([dog_data['kleekai'], dog_data['husky'], dog_data['malamute']]).T\n",
    "\n",
    "# Applying our equations\n",
    "def MSTr(data):\n",
    "    J, I = data.shape\n",
    "    grand_mean = np.mean(data)\n",
    "    group_mean_list = np.mean(data, axis=0)\n",
    "    diffs = np.array([group_mean - grand_mean for group_mean in group_mean_list])\n",
    "    MSTr = (J/(I-1))*np.sum(np.square(diffs))\n",
    "    return MSTr\n",
    "\n",
    "def MSE(data):\n",
    "    J, I = data.shape\n",
    "    group_mean_list = np.mean(data, axis=0)\n",
    "    group_diffs = (1/(J-1))*np.sum(np.square(np.subtract(data, group_mean_list)), axis=0)\n",
    "    MSE = (1/I)*np.sum(group_diffs)\n",
    "    return MSE\n",
    "\n",
    "def f_statistic(data):\n",
    "    return MSTr(data)/MSE(data)\n",
    "\n",
    "print(\"f:\", f_statistic(dog_data))\n",
    "\n",
    "# scipy.stats\n",
    "print(\"f:\", stats.f_oneway(dog_data[:, 0], dog_data[:, 1], dog_data[:, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Determine the p-value (probability)/critical value\n",
    "Given the statistic computed and the known parameters ($\\nu_{1}=2$, $\\nu_{2}=40$), we can use an [F table](https://www.stat.purdue.edu/~jtroisi/STAT350Spring2015/tables/FTable.pdf) to determine the critical value, which for our significance level of $\\alpha=0.1$ is $F_{C}=2.84$. We have also computed the corresponding p-value given the data collected, which is $p=3.06\\times10^{-20}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reject/accept null hypothesis\n",
    "Given that our p-value , which is the probability of our F statistic being observed given it follows a f distribution, is much lower than our significance level, $\\alpha=0.1$, we can reject the null hypothesis. The same conclusion can be reached by observing that our computed statistic ($F=109.39$) is much larger than the critical value observed in the F table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make a decision\n",
    "Our results show that given the observed distribution of heights for each of the three dog breeds, there is a significant difference in the population mean height of one or more of the breeds, which is to be expected from the breeds often categorised as 'small', 'large' and 'very large'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a bit more convoluted, F-tests can be very useful, and extend beyond the single-factor ANOVA. There are variants that allows us to determine which of the categories have a significantly different mean (find out more [here](https://statistics.laerd.com/statistical-guides/repeated-measures-anova-statistical-guide.php)) even the analysis of multiple independent variables simulataneously and their interactions (find out more [here]((https://www.statisticssolutions.com/conduct-interpret-factorial-anova/))). \n",
    "\n",
    "In the context of data science, just like a chi-square test can be useful in feature selection by analysing the effect of categorical variables on other categorical variables (i.e. classification problems), ANOVA can be used to analyse the effect of categorical variables on continous variables (i.e. regression problems). In our example we see that if we wanted to model dog height, then dog breed is an important variable to account for. As we will see later in the module, once we have a model, there are ANOVA functions that given our trained model, we get a p-value for each independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given the mushroom data loaded below, determine whether the mushroom being poisonous or not poisonous is significantly dependent on the feature 'cap-colour' using the framework we have described above. (use scipy.stats)\n",
    "\n",
    "Data: https://www.kaggle.com/uciml/mushroom-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading in our data\n",
    "mushroom_data = pd.read_csv(\"../DATA/mushrooms.csv\")\n",
    "mushroom_class = mushroom_data[\"class\"]\n",
    "cap_color = mushroom_data[\"cap-color\"]\n",
    "# Pre-processing data\n",
    "mushroom_class = mushroom_class.astype('category') # dependent variable\n",
    "cap_color = cap.color.astype('category') # independent variable\n",
    "class_color = pd.crosstab(cap_color, mushroom_class, margins = False) # contingency table\n",
    "# print(class_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Given the students performance data loaded below, determine whether the average overall scores of students whose parents have different levels of education are significantly different using the framework we have described above.  (use scipy.stats)\n",
    "\n",
    "Data: https://www.kaggle.com/spscientist/students-performance-in-exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading in our data (don't forget to pre-process)\n",
    "student_performance_data = pd.read_csv(\"../DATA/students_performance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
