{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KNN\r\n",
    "\r\n",
    "## Intro - the algorithm\r\n",
    "\r\n",
    "K-nearest neighbours is an extremely simple algorithm, which consists of the following steps:\r\n",
    "\r\n",
    "- take `K` nearest neighbors (nearest data points) by some metric (usually euclidean) (`K` is hyperparameter)\r\n",
    "- take average of their respective regression value (for regression tasks) __or__ do the __majority voting__ in case of labels\r\n",
    "- and you have your output!\r\n",
    "\r\n",
    "## What does it mean to be the nearest neighbour?\r\n",
    "\r\n",
    "![](images/knn_data_distances.jpg)\r\n",
    "\r\n",
    "__Note that the neighbourhood of an example in the train set includes itself!__ \r\n",
    "\r\n",
    "## Special case of model\r\n",
    "\r\n",
    "In this case our model is quite special in machine learning because:\r\n",
    "- __It has no parameters to learn__, hence it is a __non-parametric model__\r\n",
    "- __No learning phase required__ a.k.a. a __lazy predictor__\r\n",
    "- All of the data has to be kept __at all times__, hence it isn't the most memory-efficient method\r\n",
    "- Predictions are fast, but might be prone to overfitting because of `K`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "from sklearn import datasets\r\n",
    "\r\n",
    "X, y = datasets.make_blobs(\r\n",
    "    n_samples=300, centers=6, cluster_std=0.5, random_state=0\r\n",
    ")\r\n",
    "data = pd.DataFrame(\r\n",
    "    data=np.concatenate((X, y.reshape(-1, 1)), axis=1),\r\n",
    "    columns=[\"X1\", \"X2\", \"labels\"],\r\n",
    ")\r\n",
    "\r\n",
    "sns.lmplot(x='X1', y='X2', hue='labels', data=data, fit_reg=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing distance calculation\r\n",
    "\r\n",
    "![](images/knn_distance_measures.jpg)\r\n",
    "\r\n",
    "We normally use the Euclidian distance, but we may choose to run the algorithm using different distance metrics.\r\n",
    "\r\n",
    "We will use `scipy` in order to speed-up the computation and make it easier:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import scipy\r\n",
    "\r\n",
    "distances = scipy.spatial.distance.cdist(X, X)\r\n",
    "print(X.shape)\r\n",
    "print(distances.shape)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This metric will be passed to our `KNN` model as a hyperparameter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\r\n",
    "\r\n",
    "### Implementation\r\n",
    "\r\n",
    "With that in mind, we can move on to `KNN` implementation.\r\n",
    "\r\n",
    "- Create `KNN` classs taking `k` and `distance` as hyperparameters (assign `None` to `self.X` and `self.y`)\r\n",
    "- Create `fit` method taking `X` and `y` (what should it do?)\r\n",
    "- Create `predict` method taking `X` and predicting respective labels. To do that we have to:\r\n",
    "    - calculate distances between `self.X` and `X` using `self.metric`\r\n",
    "    - do sorting by index ([`np.argsort`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html)) along specific axis. Output shape from this step should be `(self.X.shape[0], X.shape[0])` so the distance of each point of `self.X` to every other point in `X`.\r\n",
    "    - Choose at most `k` samples from `self.X` (__tip: simply slice with [: self.k]__) Output shape from this step should be `(K, X.shape[0])` a.k.a. (number of neighbors, number of examples in X)\r\n",
    "    - Use `numpy`'s fancy indexing on `labels` (`self.y`) using those sorted indices. __Tip: try simplest solution__. Output shape should be the same as in previous steps\r\n",
    "    - Count how many labels `k` has for each example using `bincount2d`. Output shape should be: `(X.shape[0], classes)`, where `classes` is the number of unique classes in `y`. Can you pass output from previous step directly or do we have to transform it somehow in order for shapes to be right?\r\n",
    "    - Finally, return the label occuring most frequently using [`np.argmax`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) along specific axis. Output shape should be `(X.shape[0],)` (vector containing labels for each example)\r\n",
    "    \r\n",
    "### Analysis\r\n",
    "\r\n",
    "Check the accuracy on training dataset to be sure everything works correctly (you can use `sklearn.metrics` module).\r\n",
    "\r\n",
    "- What is the accuracy in this case and why is that?\r\n",
    "- What would we need to do to change it __for the worse__ (by only varying hyperparameters)?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import typing\r\n",
    "import dataclasses\r\n",
    "\r\n",
    "\r\n",
    "def bincount2d(x):\r\n",
    "    N = x.max() + 1\r\n",
    "    ids = x + (N * np.arange(x.shape[0]))[:, None]\r\n",
    "    return np.bincount(ids.ravel(), minlength=N * x.shape[0]).reshape(-1, N)\r\n",
    "\r\n",
    "@dataclasses.dataclass\r\n",
    "class KNN:\r\n",
    "    k: int\r\n",
    "    metric: typing.Callable[[np.array], np.array]\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.X = X\r\n",
    "        self.y = y\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        assert hasattr(self, \"X\"), \"fit method should be called before predicting!\"\r\n",
    "        distances = self.metric(self.X, X)\r\n",
    "        labels_indices = np.argsort(distances, axis=0)[: self.k]\r\n",
    "        labels = y[labels_indices]\r\n",
    "        frequencies = bincount2d(labels.T)\r\n",
    "        return np.argmax(frequencies, axis=1)\r\n",
    "\r\n",
    "\r\n",
    "        \r\n",
    "clf = KNN(k=3, metric=scipy.spatial.distance.cdist)\r\n",
    "clf.fit(X, y)\r\n",
    "clf.predict(X)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "accuracy_score(clf.predict(X), y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Numba\r\n",
    "\r\n",
    "[`numba`](https://numba.pydata.org/) is a simple Python framework which the authors describe as:\r\n",
    "\r\n",
    "> Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code.\r\n",
    "\r\n",
    "It's goal is to make our code as fast as `numpy` (or even faster) while allowing us to use Python native functions (like loops, if statements etc.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install numba"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import contextlib\r\n",
    "import time\r\n",
    "\r\n",
    "import numba\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "@contextlib.contextmanager\r\n",
    "def timer(function):\r\n",
    "    start = time.time()\r\n",
    "    yield\r\n",
    "    print(f\"Elapsed time for {function.__name__}: {(time.time() - start)}\")\r\n",
    "\r\n",
    "\r\n",
    "@numba.jit(nopython=True)  # @njit is the same\r\n",
    "def numba_trace(a):\r\n",
    "    trace = 0.0\r\n",
    "    for i in range(a.shape[0]):\r\n",
    "        trace += np.tanh(a[i, i])\r\n",
    "    return a + trace\r\n",
    "\r\n",
    "\r\n",
    "def python_trace(a):\r\n",
    "    trace = 0.0\r\n",
    "    for i in range(a.shape[0]):\r\n",
    "        trace += np.tanh(a[i, i])\r\n",
    "    return a + trace\r\n",
    "\r\n",
    "\r\n",
    "def numpy_trace(a):\r\n",
    "    return a + np.trace(a)\r\n",
    "\r\n",
    "\r\n",
    "x = np.arange(1000000).reshape(1000, 1000)\r\n",
    "\r\n",
    "# Pure Python run\r\n",
    "with timer(python_trace):\r\n",
    "    python_trace(x)\r\n",
    "\r\n",
    "# Pure numpy run\r\n",
    "with timer(numpy_trace):\r\n",
    "    python_trace(x)\r\n",
    "\r\n",
    "# First run is slower due to compilation\r\n",
    "with timer(numba_trace):\r\n",
    "    numba_trace(x)\r\n",
    "\r\n",
    "# Now it is the fastest\r\n",
    "with timer(numba_trace):\r\n",
    "    numba_trace(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## About numba\r\n",
    "\r\n",
    "`numba` is mostly about using decorators over functions (or classes in some cases), hence it's pretty easy to use.\r\n",
    "\r\n",
    "__Sometimes you need to dig a little why some code snippet does not work, but in the end it's usually worth it!__\r\n",
    "\r\n",
    "### Compilation phase\r\n",
    "\r\n",
    "- First time `numba` with `njit` decorator is run, `numba` reads Python bytecode, analyzes, optimizers and finally compiles it using [LLVM](https://llvm.org/) \r\n",
    "- Generated machine code is tailored to your specific CPU architecture (specific low-level instructions)\r\n",
    "\r\n",
    "### Tips\r\n",
    "\r\n",
    "- Use `numba` when `numpy` code is really hard to vectorize (__take some time to come up with vectorized solution always, this is your last resort__)\r\n",
    "- Use `numba` for functions which either:\r\n",
    "    - take long to run (so the compilation time does not impact the runtime)\r\n",
    "    - are run many times\r\n",
    "- Take care of arguments and their type specification (next notebook)\r\n",
    "- Use `njit` whenever possible\r\n",
    "- Numba provides `parallel` argument for decorators (for `njit` also). Use it when single loop iteration takes a long time and is independent from the next run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Voting\r\n",
    "\r\n",
    "What we have seen above is called __majority voting__:\r\n",
    "\r\n",
    "> In majority voting the label which occurs the most frequently is chosen\r\n",
    "\r\n",
    "That's why `K` is usually chosen to be an odd number in order to avoid conflicts (like `2` votes for one label and `2` for another).\r\n",
    "\r\n",
    "## Weighted majority voting\r\n",
    "\r\n",
    "> Weighted majority voting occurs when we assign weight for each example and take into an account\r\n",
    "\r\n",
    "Weights are given based on many (often different) factors (based on what our end goal is). __In case of KNN it makes sense to use weights based on similarity of provided `X` examples to the ones we have trained on__.\r\n",
    "\r\n",
    "We have calculated similarity based on euclidean distance, __but please notice those are not directly used during voting!__. \r\n",
    "\r\n",
    "### Theoretical example\r\n",
    "\r\n",
    "Imagine we have set `K=5` and let's consider single `test` example:\r\n",
    "- Let's assume one example from training has euclidean distance to our `test` example equal to `0.1`\r\n",
    "- Let's assume this example has label `0`\r\n",
    "- Now let's imagine distances for `4` other training samples to be, say, `1000` (so the samples are really not similar)\r\n",
    "- Let's assume those examples have label `1`\r\n",
    "- __Majority voting would give this example a label of `1`!__\r\n",
    "\r\n",
    "If we were to do \"weighted voting\" the weight for a single example would probably be large enough (in comparison) to change the `test` example label to `0` (which is most probably correct in this example)\r\n",
    "\r\n",
    "## Exercise\r\n",
    "\r\n",
    "Given that, let's see what steps are needed in order to implement weighted `KNN`.\r\n",
    "\r\n",
    "__We will use `numba` to make our life easier!__\r\n",
    "\r\n",
    "- __Hacky tip:__ Take specific routines out of the class and implement them separately as helpers as one can see below\r\n",
    "\r\n",
    "### Implement `_weighted_frequencies`\r\n",
    "\r\n",
    "`_weighted_frequencies` gets three arguments:\r\n",
    "- `result_array` of shape `(M, L)`, where `M` is the number of examples, `L` number of unique labels. It is filled with `zeros`\r\n",
    "- `labels` - of shape `(M, K)`, where `K` is the number of neighbors. Each value in `K` dimension is respective `KNN` label\r\n",
    "- `weights` - of shape `(M, K)`. Each value in `K` dimension is the weight given to `K`-th neighbor\r\n",
    "\r\n",
    "Your task is to, using two nested loops, sum weights for specific neighbors within `result_array` and return it (__tip:__ as those are zeros you can simply add appropriate weights at appropriate index).\r\n",
    "\r\n",
    "### Analyse the results\r\n",
    "\r\n",
    "- How the performance changes when you change `njit` to `jit` or when you remove the decorator totally?\r\n",
    "- What can we do to get non `1.0` accuracy when evaluating on `training` dataset (no, you cannot sabotage the implementation, only hyperparameters are there to vary)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "\r\n",
    "@numba.njit\r\n",
    "def _weighted_frequencies(result_array, labels, weights):\r\n",
    "    for row in range(labels.shape[0]):\r\n",
    "        for column in range(labels.shape[1]):\r\n",
    "            result_array[row, labels[row, column]] += weights[row, column]\r\n",
    "\r\n",
    "    return result_array\r\n",
    "\r\n",
    "\r\n",
    "class WeightedKNN(KNN):\r\n",
    "    def predict(self, X):\r\n",
    "        distances = self.metric(self.X, X)\r\n",
    "        labels_indices = np.argsort(distances, axis=0)[: self.k]\r\n",
    "        labels = y[labels_indices].T\r\n",
    "        weights = 1 / (np.sort(distances, axis=0)[: self.k] + 1e-7).T\r\n",
    "        result_array = np.zeros((labels.shape[0], np.max(labels) + 1))\r\n",
    "        w_frequencies = _weighted_frequencies(result_array, labels, weights)\r\n",
    "        return np.argmax(w_frequencies, axis=1)\r\n",
    "\r\n",
    "\r\n",
    "clf = WeightedKNN(k=3, metric=scipy.spatial.distance.cdist)\r\n",
    "clf.fit(X, y)\r\n",
    "\r\n",
    "print(\"With compilation phase:\")\r\n",
    "with timer(WeightedKNN):\r\n",
    "    clf.predict(X)\r\n",
    "\r\n",
    "print(\"Compiled predict:\")\r\n",
    "with timer(WeightedKNN):\r\n",
    "    clf.predict(X)\r\n",
    "\r\n",
    "\r\n",
    "accuracy_score(clf.predict(X), y)\r\n",
    "\r\n",
    "# No njit\r\n",
    "\r\n",
    "# With compilation phase:\r\n",
    "# Elapsed time for WeightedKNN: 0.07559394836425781\r\n",
    "# Compiled predict:\r\n",
    "# Elapsed time for WeightedKNN: 0.054709672927856445\r\n",
    "\r\n",
    "# njit\r\n",
    "\r\n",
    "# With compilation phase:\r\n",
    "# Elapsed time for WeightedKNN: 0.23263049125671387\r\n",
    "# Compiled predict:\r\n",
    "# Elapsed time for WeightedKNN: 0.0071125030517578125"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Limitations of K-nearest neighbours\r\n",
    "\r\n",
    "- We need to find the distance between each point and every other point. The time complexity of the algorithm is dominated by this process\r\n",
    "- Examples that might be close in feature space, may not necessarily be close in label space. \r\n",
    "    - E.g. if examples have similar feature values for features that do not influence the output label, they will be close in feature space, but not in label space. \r\n",
    "    - proximity assumption\r\n",
    "- When working with high dimensional data, it will not be easy to visualise the data and hand pick a suitable k (but we can still use `grid search` or a-like hyperparameter tuning methods)\r\n",
    "- when making predictions, we need to store the whole dataset in the model. This can make memory a bottleneck.\r\n",
    "- for the best results we should always scale our features to prevent any one with large values disproportionately influencing the prediction. But with KNN this can affect the distances between each example along each dimension of the feature space, resulting in different nearest neighbours. Try with and without feature scaling.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-nearest neighbours for regression\r\n",
    "\r\n",
    "K-nearest neighbours can also perform regression as well as classification.\r\n",
    "\r\n",
    "The only differences are:\r\n",
    "- Labels are not integer class labels, but consist of continuous values\r\n",
    "- Instead of majority voting we will simply take the mean of values (possibly weighted mean)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenges\r\n",
    "\r\n",
    "- Read [seaborn introduction](https://seaborn.pydata.org/introduction.html) in order to improve your Python plotting skills\r\n",
    "- Go through [5 minutes guide for `numba`](https://numba.readthedocs.io/en/stable/user/5minguide.html) and roam around their documentation to get a feel what one can achieve using this tool. [This document](https://numba.pydata.org/numba-doc/dev/user/jit.html) is also helpful and full of numba tips\r\n",
    "- Implement distance metrics using `numpy` only\r\n",
    "- Check how KNN works for different types of hyperparameters (use `grid search` or similar technique, remember about splitting data & K-Fold) and maybe different datasets? Experiment!\r\n",
    "- Implement K-nearest neighbours for regression (including weighted version)\r\n",
    "- Why we shouldn't use `assert` and instead go for `if`-style checking? __Tip:__ (asserts do not run in all cases)\r\n",
    "- Can you write `WeightedKNN` without using `numba` and loops (pure `numpy` implementation)?\r\n",
    "- __ADDITIONAL:__ What are the other ways for classification voting? See [this notebook](https://www.kaggle.com/amrmahmoud123/1-guide-to-ensembling-methods) for ensembles (which is multiple models assembled together to give one prediction). Don't focus on the algorithms, focus on the ideas like `blending` and `stacking` (rest is out of the scope of this material, at least for now).\r\n",
    "\r\n",
    "## Summary\r\n",
    "- The K-nearest neighbours algorithm makes predictions by averaging the labels of the K-nearest neighours in feature space.\r\n",
    "- K is the number of neighbours which the algorithm will average the labels of. It is a hyperparameter."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}