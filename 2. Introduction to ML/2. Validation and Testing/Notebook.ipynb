{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Test\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "Understand what terms below are all about:\n",
    "\n",
    "- cross validation\n",
    "- test set\n",
    "- random seeding\n",
    "- data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "\n",
    "First of all, __what is validation set?__ Simply put:\n",
    "\n",
    "> some part (some percentage) of our training data left out of training dataset\n",
    "\n",
    "Why would we leave it, isn't data precious and as many samples should be used for training as possible (so you guys said last time)?\n",
    "\n",
    "Yes, but __you should always have at least a validation set (preferably test or even more splits as we'll see in the next notebook)__. We will use it (validation) to see how our model performs on unseen data __and choose the best one according to it__.\n",
    "\n",
    "__Remember:__ Our end goal is creating algorithms which works on __new data__, __training on the training set is only a means to an end__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set\n",
    "\n",
    "The story with the test set is a little bit different. It is also some part of the data left out of training, __but with respect to which we make no decisions about our algorithm__. \n",
    "\n",
    "- It is simply informative and is an approximation of how our model will do \"in the wild\"\n",
    "- __In practice we may have no access to a real life test set__\n",
    "- As it __cannot__ affect our decision process we usually optimize model based on validation set only (or variations of it which you gonna see later)\n",
    "\n",
    "Say we trained our model and want it running for questions users post on the website. Questions coming from those users will be our real test set.\n",
    "\n",
    "__In this notebook we will use test set__ the way one always should: just to check how our model does, whether it works fine at all. We should __never optimize according to it__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn split\n",
    "\n",
    "We will train Linear regression model once again. Our focus will be on data, not on the algorithm as this technique is used for most models you are going to encounter.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "- Load Boston dataset from `sklearn` (as previously)\n",
    "- Use [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to get `X` and `y` split into `X_train, X_test, y_train, y_test` (use `0.3` for `test_size`)\n",
    "- Do it once more on `X_test` and `y_test` to get `X_test, X_validation, y_test, y_validation` (split testing part of data in half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "\n",
    "print(f\"Number of samples in dataset: {len(X)}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(\n",
    "    X_test, y_test, test_size=0.5\n",
    ")\n",
    "\n",
    "print(\"Number of samples in:\")\n",
    "print(f\"    Training: {len(y_train)}\")\n",
    "print(f\"    Validation: {len(y_validation)}\")\n",
    "print(f\"    Testing: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, 10-40% of data is left out of training either for validation or for test set also.\n",
    "\n",
    "The less data for validation, the less reliable will our estimates be, but more training samples will allow our algorithm to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing validation\n",
    "\n",
    "You probably know there are a lot of machine learning algorithms. How can we know and test which one should be used for our problem?\n",
    "\n",
    "Besides __domain knowledge about specific problem__, cross-validation can always help us. Let's train a few algorithms and choose the best based on validation set performance.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "- Some setup is already in place, try to stick to it (or not, experimenting is always a good thing)\n",
    "- Create all the models inside list. Pass `splitter=random` to `DecisionTreeRegressor`\n",
    "- Inside loop do the following:\n",
    "    - fit model in training dataset\n",
    "    - predict on training features\n",
    "    - predict on validation features\n",
    "    - predict on test features\n",
    "    - calculate `mean_squared_error` on all of the predictions\n",
    "- You can rewrite `print` statement or leave as it is, just remember about correct variable names in your loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML algorithms you will later know, don't panic\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "models = [DecisionTreeRegressor(splitter=\"random\"), SVR(), LinearRegression()]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_loss = mean_squared_error(y_train, y_train_pred)\n",
    "    validation_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "    test_loss = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    print(\n",
    "        f\"{model.__class__.__name__}: \"\n",
    "        f\"Train Loss: {train_loss} | Validation Loss: {validation_loss} | \"\n",
    "        f\"Test Loss: {test_loss}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "As you can see, best Validation Loss is for Linear Regression. This is the model we should choose. Unfortunately it occurs that on \"real\" (test) data it performs worse than Decision Tree.\n",
    "\n",
    "Once again: usually we won't have information from test loss, but now you should know this technique is imperfect (we will see how to mitigate those effects later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding\n",
    "\n",
    "Above we had this line: `np.random.seed(2)`. It is actually really important and we should know what is going on.\n",
    "\n",
    "### Pseudo-random number generators\n",
    "\n",
    "Many machine learning algorithms use random initialization (for example to instantiate the parameters of a linear regression model). Depending on algorithm it might have more or less severe effect on the result.\n",
    "\n",
    "- Each time you run algorithm based on the randomness the result may vary to some degree\n",
    "- Random number generators use so called `seed` which is a numerical value which determines what values will be generated\n",
    "- For each run to be the same (or to show some phenomenon like we did above) we should __always__ seed all functions using random numbers\n",
    "\n",
    "The last one is pretty easy in `numpy` and `sklearn` as it is a single line. Seeding this way is present in most of the frameworks.\n",
    "\n",
    "### Why seed?\n",
    "\n",
    "- When you want your experiments to be reproducible (especially important in Machine Learning)\n",
    "- To be sure the outcome will not change during each run\n",
    "\n",
    "So do it, it is good practice :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Leakage\n",
    "\n",
    "This is another deadly sin and you have to __avoid it all cost__.\n",
    "\n",
    "> Data leakage happens when the same data (or part of it) is used for both training and validation\n",
    "\n",
    "### Data Leakage examples\n",
    "\n",
    "- Normalizing images by counting mean and variance across __whole dataset__ instead of __training dataset only__. This gives the model information about mean and variance from validation dataset which may artificially boost it's performance\n",
    "- Bad data splitting - some samples are both in training and validation\n",
    "\n",
    "Let's see the last example in action...\n",
    "\n",
    "## Exercise\n",
    "\n",
    "- Create a function `calculate_validation_loss` which does the following:\n",
    "    - Creates `LinearRegression` model\n",
    "    - Fits it to training data (`X_train` and `y_train`)\n",
    "    - Predicts on validation data and calculates MSE (mean squared error)\n",
    "    - Prints validation loss to CLI\n",
    "    \n",
    "Run the rest of this cell and see what results you observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_loss(X_train, y_train, X_validation, y_validation):\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Without data leakage, train on train, validate on validation\n",
    "    model.fit(X_train, y_train)\n",
    "    y_validation_pred = model.predict(X_validation)\n",
    "    validation_loss = mean_squared_error(y_validation, y_validation_pred)\n",
    "\n",
    "    print(f\"Validation loss: {validation_loss}\")\n",
    "    \n",
    "# Without data leakage, train on train, validate on validation\n",
    "calculate_validation_loss(X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "# With data leakage, 50 samples from validation added\n",
    "fail_X_train = np.concatenate((X_train, X_validation[:50]))\n",
    "fail_y_train = np.concatenate((y_train, y_validation[:50]))\n",
    "\n",
    "calculate_validation_loss(fail_X_train, fail_y_train, X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, as the model saw part of validation data and it __falsely__ performs better on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- Change seed values and see how this affects our results\n",
    "- Write at least one additional downside of using validation set based on above experiments\n",
    "- How the results change based on `test_train_split` values? What happens if you give more/less data for training/validation/test phase? Experiment and make a report with a few sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Validation set is used to find info about best algorithms, best set of arguments to algoirthms etc.\n",
    "- Test set is used to check how our algorithm performs on unseen data\n",
    "- __As we tune algorithms according to `validation` dataset we cannot use it to check performance__\n",
    "- `seed` is used to ensure reproducibility. Also multiple runs for experiments are good if our code depends on random initialization heavily (we can take mean results of experiments)\n",
    "- Data leakage is information from `validation` (or `test`) leaking into training\n",
    "- Data leakage leads to falsely good results and should be avoided\n",
    "- Rule of thumb: imagine you only have training dataset when doing preprocessing. Anything you calculate from it cannot be used in `validation` or `test`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}