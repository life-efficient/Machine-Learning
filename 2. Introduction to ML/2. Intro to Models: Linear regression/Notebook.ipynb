{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Linear regression\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic Python\n",
    "- Linear algebra\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "- Know the difference between monovariate and multivariate regressions\n",
    "- Implement your first machine learning algorithm from scratch, in Python\n",
    "- Use analytical solution to solve for it\n",
    "- See how to optimize linear regression using analytical solution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading dataset\n",
    "\n",
    "Once again we will use `Boston` dataset from `sklearn`, we saw it previously, easy stuff by now. Let's also split it into validation and test:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn import datasets, model_selection\n",
    "\n",
    "# 15% for validation and test, 70% for train in total\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_validation, X_test, y_validation, y_test = model_selection.train_test_split(\n",
    "    X_test, y_test, test_size=0.5\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(354, 13) (354,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is linear regression?\n",
    "\n",
    "Classic starting point for machine learning adventures, something like `Hello World` but in ML world.\n",
    "\n",
    "__Linear regression predicts continuous outputs__ - hence the regression part of the name.\n",
    "Linear regression makes predictions that are simply a __`w`eighted__ combination (a linear combination) of the inputs (plus some offset called __`b`ias__). It is described by linear function:\n",
    "\n",
    "$$\n",
    "    y = wx + b\n",
    "$$\n",
    "\n",
    "\n",
    "![](images/linear_model.jpg)\n",
    "\n",
    "In future we will experience much more complex, nonlinear relationships between features and labels that we wish to model. \n",
    "\n",
    "But __do not underestimate linear regression__ as it is often used in statistics and to explain a lot of phenomenas, at the end of the lesson we will see when it should be used in real world.\n",
    "\n",
    "Functions that a model represent are often referred to as the **hypothesis**.\n",
    "\n",
    "![](images/linear_model_example.jpg)\n",
    "\n",
    "We will make our model able to make predictions for many examples at a time by expressing the hypothesis in vector form as shown below.\n",
    "\n",
    "![](images/linear_model_vector.jpg)\n",
    "\n",
    "Here's an example of what that computation might look like numerically.\n",
    "\n",
    "![](images/linear_model_vector_example.jpg)\n",
    "\n",
    "\n",
    "## Mathematical formula of model\n",
    "\n",
    "Formula below presents linear regression for single example __but multiple features__:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y = w_1x_1 + w_2x_2 + ... + w_Nx_N + b = \\sum_{i=1}^{N} w_ix_i + b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Essentially:\n",
    "- each feature in our sample is multiplied by bias\n",
    "Now let's implement our first machine learning model in code!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple features\n",
    "\n",
    "We will go for multiple features, so here is how our weights will look like:\n",
    "\n",
    "![title](images/w_vector.jpg)\n",
    "\n",
    "The weights variable (w) becomes a row vector so we need to transpose it when we multiply it by the X matrix (or take a `dot` product of `data` and `weights`).\n",
    "\n",
    "![title](images/vector_linear_regression.jpg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monovariate vs multivariate\n",
    "\n",
    "A dychotomy you might sometimes come across:\n",
    "\n",
    "> monovariate linear regression is linear regression done with __one or multiple features__ but __predicting single target__\n",
    "\n",
    "And __multivariate__ (as you may of guessed) would be\n",
    "\n",
    "> linear regression with __one or multiple variables (features)__ but __predicting multiple targets__ (which are correlated with each other)\n",
    "\n",
    "In this notebook we will be doing __monovariate__ only, but we will get to __multivariate__ when we do multiclass classification."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\n",
    "\n",
    "`LinearRegression` implementation is our task!\n",
    "\n",
    "- Create a class `LinearRegression` which takes a single `n_features` argument during initialization.\n",
    "    - Create `W` and `b` variables inside initialization. One of shape `(n_features, 1)` and `bias` of shape `1` initialized with random normal distribution\n",
    "- Create `__call__` function (what does it do, what is a functor?) which takes `X` (`np.array`). It should return predictions our linear regression should do (see formulas above in the picture, it's two operations only)\n",
    "- Create `update_params` function which takes `W` and `b` and assigns them to appropriate variables in `self`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, n_features: int): # initalize parameters \n",
    "        self.W = np.random.randn(n_features, 1) ## randomly initialise weight\n",
    "        self.b = np.random.randn(1) ## randomly initialise bias\n",
    "        \n",
    "    def __call__(self, X): # how do we calculate output from an input in our model?\n",
    "        ypred = np.dot(X, self.W) + self.b\n",
    "        return ypred # return prediction\n",
    "    \n",
    "    def update_params(self, W, b):\n",
    "        self.W = W ## set this instance's weights to the new weight value passed to the function\n",
    "        self.b = b ## do the same for the bias"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model = LinearRegression(n_features=13)  # instantiate our linear model\n",
    "y_pred = model(X_train)  # make prediction on data\n",
    "print(\"Predictions:\\n\", y_pred[:10]) # print first 10 predictions"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predictions:\n",
      " [[ 8.53231141e-03]\n",
      " [-5.35468431e+00]\n",
      " [ 5.95418505e+00]\n",
      " [ 3.35656766e+00]\n",
      " [ 4.51060464e+00]\n",
      " [-2.13382115e+00]\n",
      " [-1.02723330e+00]\n",
      " [-8.65783812e+00]\n",
      " [ 1.58804725e+00]\n",
      " [ 1.17156681e+00]]\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(y_pred, y_true):\n",
    "    samples = len(y_pred)\n",
    "    plt.figure()\n",
    "    plt.scatter(np.arange(samples), y_pred, c='r', label='predictions')\n",
    "    plt.scatter(np.arange(samples), y_true, c='b', label='true labels', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Sample numbers')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "plot_predictions(y_pred[:10], y_train[:10])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis\n",
    "\n",
    "As you can see predictions of our model are __way off__. This happens because we initialized our model with random weights and bias.\n",
    "\n",
    "Now, we should learn how we can improve this model to learn from data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Loss - how do we know how good our model is?\n",
    "\n",
    "> Our **loss** should measure __how poor our model performs__. \n",
    "\n",
    "The larger the value, the worse so we will later try to __minimize it__ (bring as close to zero as possible). We will use it to give our model feedback about it's performance. \n",
    "\n",
    "> Loss values needs to return a **single number**, not a vector, not a matrix.\n",
    "\n",
    "__NOTE:__ minimising the objective is equivalent to maximising the negative of it. \n",
    "\n",
    "Commonly, loss value is also called __cost function__ though it is not exact. Let's go over the difference now.\n",
    "\n",
    "### Squared Error loss\n",
    "\n",
    "> loss is a function which takes prediction and true label and returns __a positive scalar__\n",
    "\n",
    "- The higher the loss value, the worse our model performs\n",
    "- __Loss is defined on a single data point__\n",
    "\n",
    "Squared error is one of the loss functions __used for regression tasks__ and is simply defined as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    (\\hat{y} - y)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This does exactly what you think: it calculates the error (difference between our model's prediction $\\hat{y}$ and the true value $y$):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\hat{y} - y\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "and then squares it to make the value positive. As long as the error is not zero it will increase the value of loss regardles of whether our prediction is below (negative error) or above (positive error) the value of the label.\n",
    "\n",
    "### Mean Squared Error (MSE) cost function\n",
    "\n",
    "> cost function is a generalization of loss functions for many data samples\n",
    "\n",
    "So, __loss__ operates on single sample, while __cost__ operates on multiple of them.\n",
    "In case of __Mean Squared Error__ we calculate squared error for each sample and take the mean of that value:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    L_{mse} = \\frac{1}{N}\\sum_{i}^{N}(\\hat{y_i} - y_i)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "There are many other criterions that are useful for different tasks (e.g. the binary cross entropy (BCE) loss for classification, which we will cover later).\n",
    "\n",
    "Let's write a function to calculate the cost using the mean squared error loss function. It should take in an array of predictions for different example inputs as well as an array of corresponding example labels. It should return a single number (scalar) that represents the MSE loss. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\n",
    "\n",
    "Implement `mean_squared_error` function taking `y_pred` and `y_true`. Every formula is above (focusing on the last one is enough ;) )"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def mean_squared_error(y_pred, y_true):  # define our criterion (loss function)\n",
    "    errors = y_pred - y_true  ## calculate errors\n",
    "    squared_errors = errors ** 2  ## square errors\n",
    "    return np.mean(squared_errors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "cost = mean_squared_error(y_pred, y_train)\n",
    "print(cost)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "202773.00625866305\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The analytical solution to minimising mean square error\n",
    "\n",
    "Now that we have our __loss__ equation we can calculate it's derivative w.r.t. weights. When we set it to zero we can calculate __weights values (`W`)__ which minimize it.\n",
    "\n",
    "![](images/analytical_linear_reg.jpg)\n",
    "\n",
    "Now let's implement this analytical solution for least squares regression in code:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\n",
    "\n",
    "Now that we have mathematical formula we can jump in straight to the implementation.\n",
    "\n",
    "- For matrix inverse, you can use [`np.linalg.inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) function\n",
    "- Remember to return `weights` part of `matrix` first and `bias` after that (`bias` is the `0` element of the result)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def minimize_loss(X_train, y_train):\n",
    "    X_with_bias = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "    optimal_w = np.matmul(\n",
    "        np.linalg.inv(np.matmul(X_with_bias.T, X_with_bias)),\n",
    "        np.matmul(X_with_bias.T, y_train),\n",
    "    )\n",
    "    return optimal_w[1:], optimal_w[0]\n",
    "\n",
    "\n",
    "weights, bias = minimize_loss(X_train, y_train)\n",
    "print(weights, bias)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-1.22314495e-01  4.63139012e-02  2.94029510e-02  2.13765293e+00\n",
      " -1.59495784e+01  4.28401974e+00 -1.21465182e-02 -1.41230082e+00\n",
      "  2.75826409e-01 -1.28000582e-02 -9.77786824e-01  9.85027744e-03\n",
      " -4.05834430e-01] 32.264564811607215\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In case you didn't notice, this analytical solution has no mention of the model bias. \n",
    "In fact, we incorporate the model bias into our features matrix by adding an extra column filled with `1`.\n",
    "\n",
    "![](images/bias_in_weight_matrix.jpg)\n",
    "\n",
    "Doing this makes the analytical solution much clearer and means we have to solve it only for one value $W$, rather than also for $b$.\n",
    "\n",
    "In practice (iterative optimization), we treat them as separate variables (we will later see more about that)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Update parameters\n",
    "\n",
    "Now that we have found `optimal_w` we should update our model and see how it performs:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model.update_params(weights, bias)\n",
    "y_pred = model(X_train)\n",
    "cost = mean_squared_error(y_pred, y_train)\n",
    "print(cost)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20.912899991972914\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drawbacks of computing the analytical solution\n",
    "\n",
    "This solution involves inverting a matrix of size $R^{n \\times n}$. \n",
    "Here $n$ is the number of features that each example has. \n",
    "\n",
    "With `560` features it is becoming more difficult. Furthermore, here, we only have ~500 samples, while in real life we can have millions or more.\n",
    "\n",
    "However, as we will see, most problems of practical interest contain examples with many more features. \n",
    "\n",
    "> For example, 1080p images have more than 1,000,000 features each. \n",
    "\n",
    "The time complexity of inverting a matrix of size $n \\times n$ is around $O(n^3)$. \n",
    "This means that computing the analytical solution for these kinds of real world problems is often computationally expensive or even impossible.\n",
    "\n",
    "Analytical solutions however, are not the only approach that we can take (and usually we __even cannot use them__ as the close form cannot be calculated).\n",
    "\n",
    "We will see how to update parameters iteratively soon."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "- linear regression is \"hello world\" basic machine learning model\n",
    "- linear regression updates it's weight vector and bias in order to improve on the task\n",
    "- this update can be carried out via analytically calculated formula\n",
    "- the MSE loss is appropriate for many regression problems and is the most common loss function for this task"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}