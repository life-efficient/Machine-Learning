{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what EDA is, and why it is necesary\n",
    "- Practise applying various visualisation techniques\n",
    " - Learn what type of plot is appropiate for what situation\n",
    " \n",
    "EDA stands for **E**xploratory **D**ata **A**nalysis and is a critical precursor to applying a model. As the name implies, it is all about exploring your data - validating that the dataset you'll be working on is clean, and without missing values. Perhaps most interestingly, however, is the ability to use various visualisation techniques on our data to gain an understanding of underlying trends between the variables provided.\n",
    "\n",
    "I want to note that not _all_ problems you will come across require the use of a model. Perhaps the task at hand is to \"simply\" provide visualisations and identify interesting facts which could not be done through a non-visual analysis. \n",
    "When we do want to use a model, it is important to have our hypothesis formulated. This is because the identification of what you're trying to find will be relevant in determining what parts of your data you explore. Either way, we'll be visualising data. To expand on why this is necessary, the image below demonstrates something known as *Anscombe's Quartet*:\n",
    "<img src=\"https://www.researchgate.net/profile/Arch_Woodside2/publication/285672900/figure/fig4/AS:305089983074309@1449750528742/Anscombes-quartet-of-different-XY-plots-of-four-data-sets-having-identical-averages.png\">\n",
    "([source](https://www.researchgate.net/publication/285672900_The_general_theory_of_culture_entrepreneurship_innovation_and_quality-of-life_Comparing_nurturing_versus_thwarting_enterprise_start-ups_in_BRIC_Denmark_Germany_and_the_United_States))\n",
    "\n",
    "Ascomebe's Quartet is case in point why visualising our data is of upmost importance. The image shows us that the summary statistics (e.g. mean, variance) for all the data is the same. However, as can be seen, the distributions which the data come from are wildly different. Had we not visualised our data, we would not have been able to trivially identify the relationships of the data.\n",
    "\n",
    "Throughout this chapter, I've introduced one or two different kinds of plots but have intentionally generally tried to minimise the number of them. I did that for teaching purposes only as I did not want to overload you guys with information. In reality, data cleaning and dealing with missing data falls under the EDA umbrella, and it will be a cyclical process where you explore your data, find out things wrong with it, clean it, and explore again.\n",
    "\n",
    "The plan for the next couple of notebooks is to drill in various types of plots under different contexts (i.e. different notebooks). We'll start with something relatively straightfoward in this notebook, and will expand on complexity as we progress through the notebooks.\n",
    "\n",
    "We'll be working with the \"multiple_choice_responses.csv\" file from the [2019 Kaggle ML & DS Survey](https://www.kaggle.com/c/kaggle-survey-2019/data?select=multiple_choice_responses.csv), which is a 35 question survey performed on Kaggle users regarding the state of data science and machine learning. From their abstract, this survey received 19,717 usable respondents from 171 countries and territories. If a country or territory received less than 50 respondents, we grouped them into a group named \"Other\" for anonymity. The task that we're going to assign ourself with this dataset is to identify what factors signifcantly impact the annual salary of those in DSML.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Load the dataset and return the first few rows\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "df = pd.read_csv(\"https://aicore-files.s3.amazonaws.com/Data-Science/multiple_choice_responses.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There's a file in the DATA folder called \"questions_only.csv\". Load in the dataset and all print the questions\n",
    "q_df = pd.read_csv(\"https://aicore-files.s3.amazonaws.com/Data-Science/questions_only.csv\")\n",
    "for i, question in enumerate(q_df.iloc[0]):\n",
    "    print(i, \"\\t\", question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this preview, here is what I noticed:\n",
    "- There are a lot of questions (lots of data to analyse)\n",
    "- Some of the questions allow for multiple inputs. For these questions, the header row/column names have `_` appended to them, followed by some text.\n",
    "  - If the text is `OTHER_TEXT` then it seems to indicate that following a categorical question, a text field giving the option for the recipient to expand is provided. It looks like -1 is means that the user did not write anything.\n",
    "  - If the text is `PART_N` then it seems to be a checkbox question (i.e. tick all that apply)\n",
    "  - They are not mutually exclusive\n",
    "  \n",
    "Analysing this data column by column is going to take way too long. So we're going to decide on factors we think may influence salary, and extract the relevant questions which meet this criteria from the list. This is partially why data science is considered an art - you may find yourself with a big dataset and find yourself unsure where to start analysing it. Using your hypothesis and identifying what you're trying to model, you need to use your best intuition as to the factors you think will be heavily influential to this. This is why domain expertise is important. But the more you explore your data with the initial concepts you had in mind, the more you'll end up learning about the wider dataset.\n",
    "- Salary (target)\n",
    "- Age\n",
    "- Gender\n",
    "- Residence\n",
    "- Education\n",
    "- Job role/Experience\n",
    "- Programming languages\n",
    "- ML frameworks\n",
    "\n",
    "From this list, I'll extract these questions:\n",
    "**Q:** 1, 2, 3, 4, 5, 9, **10**, 15, 18, 24, 28.\n",
    "\n",
    "There are a couple others in there which would be relevant to analyse too - in an ideal world we would analyse them too, but time is limited here - and what is important is to teach you various visualiation techniques while building intuition as to what to look for in data. \n",
    "\n",
    "Some of these questions encompass multiple columns in our dataframe. Extracting the relevant columns that we want isn't the most straightforward task. Take some time to try and implement something which returns a new dataframe which contains the relevant columns. If you're unsure how to proceed after a couple of minutes, click below to try and implement the method that I would use.\n",
    "<details>\n",
    "    <summary><b>> Click here to find out how I would do this</b></summary>\n",
    "    <ul>\n",
    "        <li>Define a function which loops over a list of integers of questions we want to keep</li>\n",
    "        <li>For every iteration, work out the amount of columns from the current question to the next question in the dataframe (NOT the next question we want to extract)</li>\n",
    "        <li>Extract/concatenate from the current column position to the current column position + 'distance' (probably using the <code>range()</code> function from Python)</li> \n",
    "    </ul>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_keep = [1,2,3,4,5,9,10,15,18,24,28]\n",
    "\n",
    "def extract_columns(df, idx_to_keep):\n",
    "    \n",
    "    new_df = pd.DataFrame() # empty dataframe\n",
    "    df_col_list = df.columns.tolist()\n",
    "    \n",
    "    for i in idx_to_keep:\n",
    "        column_name_base = \"Q{}\".format(i)\n",
    "        column_index = [df_col_list.index(col_name) for col_name in df_col_list if col_name.startswith(column_name_base)][0]\n",
    "               \n",
    "        next_column_name_base = \"Q{}\".format(i+1)\n",
    "        next_column_index = [df_col_list.index(col_name) for col_name in df_col_list if col_name.startswith(next_column_name_base)][0]\n",
    "         \n",
    "        col_idxs_to_extract = range(column_index, next_column_index)\n",
    "        relevant_cols_df = df.iloc[:, col_idxs_to_extract]\n",
    "        \n",
    "        new_df = pd.concat([new_df, relevant_cols_df], axis=1)\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "\n",
    "df_orig = df.copy(deep=True)\n",
    "df = extract_columns(df_orig, idx_to_keep)\n",
    "df = df[1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still... a lot of data... well, we gotta start somewhere! Arbitrarily, let's start with Gender (Q2). We see that the data here is meant to be categorical, so after ensuring that's the case, let's simply plot the frequency of each of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Q2\"] = df[\"Q2\"].astype(\"category\")\n",
    "set(df[\"Q2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.histogram(df, \"Q2\", labels={\"value\": \"Gender\"}, title=\"Counts of Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! What about where the residencies of the individuals? We'll turn it up a notch and plot these on a world map, heating them by the number of respondants from that country. This is known as a [choropleth map](https://plotly.com/python/choropleth-maps/) and will require us to change our country names into [3 letter ISO codes](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3).\n",
    "\n",
    "The first thing we need to do is look at the countries column itself (i.e. Q3). After doing so, it's worth updating values to something more conventional if they're not there yet.\n",
    "\n",
    "We'll then load in a package to which we can pass a country and have it return the ISO code for us. Then we'll use the new column to plot our choropleth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[\"Q3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here are the values I think need updating:\n",
    "- Hong Kong (S.A.R.)\n",
    "- Iran, Islamic Republic of...\n",
    "- United Kingdom of Great Britain and Northern Ireland\n",
    "- Viet Nam\n",
    "- South Korea\n",
    "\n",
    "Also notice there's an \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of 'Other':\", df[\"Q3\"].value_counts()[\"Other\"]/len(df) * 100)\n",
    "\n",
    "values_to_update = {\"Q3\": \n",
    "                    {\"Hong Kong (S.A.R.)\": \"Hong Kong\",\n",
    "                     \"Iran, Islamic Republic of...\": \"Iran\",\n",
    "                     \"United Kingdom of Great Britain and Northern Ireland\": \"United Kingdom\",\n",
    "                     \"South Korea\": \"Republic of Korea\",\n",
    "                     \"Viet Nam\": \"Vietnam\"}}\n",
    "\n",
    "## Using the replace method, update the values in the relevant column\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\n",
    "df.replace(values_to_update, inplace=True)\n",
    "set(df[\"Q3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "## Create a new dataframe which will hold only the unique countries, their country codes and the number of instances of this country - WITHOUT \"Other\"\n",
    "countries = df[\"Q3\"][df[\"Q3\"]!= \"Other\"].unique()\n",
    "countries_df = pd.DataFrame(countries, columns=[\"Country\"])\n",
    "countries_df[\"Count\"] = countries_df[\"Country\"].map(df[\"Q3\"].value_counts())\n",
    "\n",
    "## Create a new column in the dataframe which has the ISO country codes\n",
    "country_codes = []\n",
    "for country in countries_df[\"Country\"]:\n",
    "    country_code = pycountry.countries.search_fuzzy(country)[0] # Take the first element returned from the search\n",
    "    country_codes.append(country_code.alpha_3)\n",
    "\n",
    "countries_df[\"Country Code\"] = country_codes\n",
    "countries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.choropleth(countries_df, locations=\"Country Code\", hover_name=\"Country\", color=\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about age by gender? We'll have to group variables together first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_gender_df = df[[\"Q1\", \"Q2\"]]\n",
    "age_gender_groups = age_gender_df.groupby([\"Q1\", \"Q2\"]).size().unstack()\n",
    "fig = px.bar(age_gender_groups, title=\"Count of Age per Gender\", labels={\"Q1\": \"Age\", \"value\": \"Count\"})\n",
    "fig.update_layout(legend_title_text='Gender')\n",
    "# fig.update_layout(barmode=\"group\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the most frequent age of DSML employees are between 25-29. I see two reasons why these values are considerbly higher than the others:\n",
    "1. Data Science and Machine Learning is a relatively new discipline, and now there exist direct education paths to these fields, which is more accessible to younger people\n",
    "2. Think about _where_ the data was collected from. Older people are perhaps less likely to use 'resource' sites like Kaggle becuase 1) They don't feel they need the learning experience and 2) Younger people are more common within social sites.\n",
    "\n",
    "So far, we've just arbitrarily produced plots - perhaps a better plan is to perform a slightly more investigative analysis over the categories we outlined earlier. Let's do this with Education.\n",
    "\n",
    "### Education Analysis\n",
    "\n",
    "Produce two plots:\n",
    "1. The participants' formal education\n",
    "2. The count of formal education per gender. Display this is a grouped bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, \"Q4\", height=800, title=\"Count of Education\", labels={\"value\": \"Education level\"})\n",
    "fig.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_gender_df = df[[\"Q2\", \"Q4\"]]\n",
    "edu_gender_groups = edu_gender_df.groupby([\"Q4\", \"Q2\"]).size().unstack()\n",
    "fig = px.bar(edu_gender_groups, title=\"Education level count per Gender\",\n",
    "             labels={\"Q2\": \"Education\", \"value\": \"Count\"},\n",
    "             height=800)\n",
    "fig.update_layout(legend_title_text='Gender', xaxis={'categoryorder':'total descending'})\n",
    "# fig.update_layout(barmode=\"group\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create another diagram showing the same information but across 4 different plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, \"Q4\", \n",
    "                   facet_col=\"Q2\", \n",
    "                   color=\"Q2\",\n",
    "                   title=\"Counts of Education level per Gender\",\n",
    "                   labels={\"Q4\": \"Education Level\"},\n",
    "                   height=1000, \n",
    "                   facet_col_wrap=2, \n",
    "                   facet_col_spacing=0.1,\n",
    "                   )\n",
    "fig.update_layout(showlegend=False, xaxis={'categoryorder':'total descending'})\n",
    "fig.update_yaxes(matches=None, showticklabels=True)\n",
    "# fig.update_xaxes(showticklabels=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting things we can establish from this... That is:\n",
    "1. Those who choose to self describe their gender are more likely have a doctorate than a bachelor's - vs every other category, which are more likely to have a bachelor's than a doctorate. Although if we note the counts, we can see that we're working with single digit figures - not something we can statistically extrapolate.\n",
    "2. Those who preferred not to give their gender also preferred not to give their education level out either (relative to the other categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I'm about to introduce next is probably one of my favourite kinds of plots. It's known as a [Sankey Diagram](https://en.wikipedia.org/wiki/Sankey_diagram).\n",
    "\n",
    "The easiest way to start working with the Sankey diagram is to understand what we want as the terminating column. In this case, we'll use education level as the final column. We'll also need 'counts' - that is - how many total people are in each level of education. To save space on the diagram, we'll generalise some of the levels.\n",
    "\n",
    "In this Sankey diagram, I want to visualise the path of gender, age, and country to education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want five levels of education: Bachelor's, Master's, Doctoral, Professional, Other\n",
    "## Create a new dataframe with just the education of surveyees where their education has been mapped to the above level\n",
    "education_df = pd.DataFrame(df[\"Q4\"])\n",
    "education_df.rename(columns={\"Q4\": \"Education Level\"}, inplace=True)\n",
    "\n",
    "values_to_update = {\"Education Level\": \n",
    "                    {\"Some college/university study without earning a bachelor’s degree\": \"Other\",\n",
    "                     \"No formal education past high school\": \"Other\",\n",
    "                     \"I prefer not to answer\": \"Other\"}}\n",
    "\n",
    "education_df = education_df.replace(values_to_update)\n",
    "set(education_df[\"Education Level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop na's from Education Level\n",
    "education_df.isna().sum()\n",
    "education_df = education_df.dropna(subset=[\"Education Level\"])\n",
    "education_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the gender, age and region columns to the new dataframe. Name the columns appropiately\n",
    "cols_to_join = [\"Q1\", \"Q2\", \"Q3\"]\n",
    "desired_col_names = [\"Age\", \"Gender\", \"Region\"]\n",
    "for col, name in zip(cols_to_join, desired_col_names):\n",
    "    education_df[name] = df[col]\n",
    "    \n",
    "education_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualisation purposes let's create:\n",
    "# 1. wider age bins as 18-29, 30-49, 50-69 and 70+\n",
    "# 2. group genders as \"Male\", \"Female\", \"Other\"\n",
    "# 3. Convert countries to continents - apart from \"India\", \"United States of America\" and \"Other\"\n",
    "\n",
    "## Overwrite the age and gender columns so that ages are now: 18-29, 30-49, 50-69 and 70+ and genders are \"Male\", \"Female\" and \"Other\"\n",
    "values_to_update = {\n",
    "    \"Age\": {\"18-21\": \"18-29\", \"22-24\": \"18-29\", \"25-29\": \"18-29\",\n",
    "            \"30-34\": \"30-49\", \"35-39\": \"30-49\", \"40-44\": \"30-49\", \"45-49\": \"30-49\",\n",
    "            \"50-54\": \"50-69\", \"55-59\": \"50-69\", \"60-69\": \"50-69\"\n",
    "           },\n",
    "    \"Gender\": {\"Prefer not to say\": \"Other\", \"Prefer to self-describe\": \"Other\"}\n",
    "}\n",
    "\n",
    "education_df = education_df.replace(values_to_update)\n",
    "education_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry_convert as pc\n",
    "## Map countries to their relevant continents, unless the country is India, United States of America, or Other\n",
    "countries_to_not_map = [\"India\", \"United States of America\", \"Other\"]\n",
    "countries_to_map_to_continents = set(education_df[\"Region\"])\n",
    "for country in countries_to_not_map:\n",
    "    countries_to_map_to_continents.discard(country)\n",
    "\n",
    "countries_continent_dict = dict()\n",
    "for country in countries_to_map_to_continents:\n",
    "    country_alpha2 = pycountry.countries.search_fuzzy(country)[0].alpha_2\n",
    "    continent_code = pc.country_alpha2_to_continent_code(country_alpha2)\n",
    "    continent_name = pc.convert_continent_code_to_continent_name(continent_code)\n",
    "    countries_continent_dict[country] = continent_name\n",
    "\n",
    "to_update = {\"Region\": countries_continent_dict}\n",
    "education_df = education_df.replace(to_update)\n",
    "education_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-indexing the columns in the order we want them for the diagram because it'll be easier to work with\n",
    "education_df = education_df.reindex([\"Gender\", \"Age\", \"Region\", \"Education Level\"], axis=1)\n",
    "\n",
    "col_names = education_df.columns.tolist()\n",
    "node_labels = []\n",
    "num_categorical_vals_per_col = []\n",
    "for col in col_names:\n",
    "    uniques = education_df[col].unique().tolist()\n",
    "    node_labels.extend(uniques)\n",
    "    num_categorical_vals_per_col.append(len(uniques))\n",
    "    \n",
    "node_labels, num_categorical_vals_per_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so having seen a Sankey diagram, we know that the previous column is only going to connect to the next column. The `num_categorical_vals_per_col` is going to allow us to know which values from the previous we need to map to the next.\n",
    "\n",
    "Now we need to construct our `link` dictionary. This is a bit less straightforward than the above. Our `link` dictionary will contain 3 lists: `source`, `target` and `value`. `source` and `target` indicate which nodes we want to connect to each other, and `value` indicates the quantity we want to 'fill' that connection with. `source` and `target` are numerical indexes of the `node_labels` list we created above.\n",
    "\n",
    "For each category per column (source category), we're going to link that category to all the other categories of the next column (target category), with the size of how many of the source categories map to the target categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_df.groupby([\"Gender\", \"Age\"]).size()[\"Female\"][\"18-29\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "value = []\n",
    "colors = []\n",
    "for i, num_categories in enumerate(num_categorical_vals_per_col):\n",
    "    \n",
    "    if i == len(num_categorical_vals_per_col)-1:\n",
    "        break\n",
    "    \n",
    "    # index allows us to refer to the categories by index from the `node_labels` list\n",
    "    start_index = sum(num_categorical_vals_per_col[:i])\n",
    "    start_index_next = sum(num_categorical_vals_per_col[:i+1])\n",
    "    end_index_next = sum(num_categorical_vals_per_col[:i+2])\n",
    "#     print(start_index, start_index_next, end_index_next)\n",
    "    \n",
    "    # i can also give us the category column to refer to\n",
    "    col_name = col_names[i]\n",
    "    next_col_name = col_names[i+1]\n",
    "    \n",
    "    grouped_df = education_df.groupby([col_name, next_col_name]).size()\n",
    "#     print(grouped_df)\n",
    "    \n",
    "    for source_i in range(start_index, start_index_next):\n",
    "        for target_i in range(start_index_next, end_index_next):\n",
    "            source.append(source_i)\n",
    "            target.append(target_i)\n",
    "            source_label = node_labels[source_i]\n",
    "            target_label = node_labels[target_i]\n",
    "            # if the index doesn't exist in the grouped_df, then the value is 0\n",
    "            try:\n",
    "                value.append(grouped_df[source_label][target_label])\n",
    "            except:\n",
    "                value.append(0)\n",
    "            \n",
    "            random_color = list(np.random.randint(256, size=3)) + [random.random()]\n",
    "            random_color_string = ','.join(map(str, random_color))\n",
    "            colors.append('rgba({})'.format(random_color_string))\n",
    "\n",
    "print(source)\n",
    "print(target)\n",
    "print(value)\n",
    "\n",
    "link = dict(source=source, target=target, value=value, color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = node_labels,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = link)])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram (Gender, Age, Region, Education)\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "\n",
    "We took a very quick look at age earlier on, but I'm interested in finding out some more things about it. Using the orignal `df`, produce the following plots:\n",
    "1. A facet plot of the count of education levels per age\n",
    "2. A facet plot of the different roles by age\n",
    "\n",
    "We will then cover the following:\n",
    "1. A plot of the count of the different languages \n",
    "2. Subplots/facet plot of the count of different languages per age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll sort the df by Age so our plot displays in the age order\n",
    "df = df.sort_values(by=[\"Q1\"])\n",
    "\n",
    "fig = px.histogram(df, \"Q4\", facet_col=\"Q1\",\n",
    "             color=\"Q1\",\n",
    "             title=\"Counts of Education level per Age\",\n",
    "             labels={\"Q1\": \"Age\", \"Q4\": \"Education Level\"},\n",
    "             height=1000, \n",
    "             facet_col_wrap=4, \n",
    "             facet_col_spacing=0.1)\n",
    "\n",
    "fig.update_layout(showlegend=False, xaxis={'categoryorder':'total descending'})\n",
    "fig.update_yaxes(matches=None, showticklabels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! The younger results are perhaps what we'd expect - 18-21 year olds typically aren't old enough to do masters degrees hence the number of bachelor's is higher for them.  However, for almost every other age group, Master's degrees are prominent. Curiously, those over 70 are more likely to have a doctorate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[\"Q5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(df, \"Q5\", facet_col=\"Q1\",\n",
    "             color=\"Q1\",\n",
    "             title=\"Counts of Education level per Age\",\n",
    "             labels={\"Q1\": \"Age\", \"Q5\": \"Job Role\"},\n",
    "             height=2000, \n",
    "             facet_col_wrap=2, \n",
    "             facet_col_spacing=0.1)\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(showticklabels=True, tickangle=45)\n",
    "fig.update_yaxes(matches=None, showticklabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data is in columns 18_p1, 18_p12\n",
    "# Our first step will be to create a new column called \"Known programming languages\", and per row, create a comma separated list which contain the programming languages they know (obviously excluding NaNs)\n",
    "programming_cols = [\"Q18_Part_{}\".format(str(i)) for i in range(1, 13)]\n",
    "programming_df = df[programming_cols]\n",
    "programming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_col = []\n",
    "for row in programming_df.itertuples(index=False):\n",
    "    languages_known = [language for language in row if isinstance(language, str)]\n",
    "    programming_col.append(\",\".join(languages_known))\n",
    "    \n",
    "programming_df[\"languages_known\"] = programming_col\n",
    "programming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's trim the new df so it only has our new col\n",
    "programming_df.drop(labels=programming_cols, axis=1, inplace=True)\n",
    "programming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume blanks mean they don't know a language and replace both the blanks and \"None\" with \"None/NA\"\n",
    "values_to_update = {\"languages_known\": {\"\": \"None/NA\", \"None\": \"None/NA\"}}\n",
    "programming_df = programming_df.replace(values_to_update)\n",
    "programming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the get_dummies method to create new columns \n",
    "language_dummies = programming_df['languages_known'].str.get_dummies(sep=',')\n",
    "language_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(language_dummies.sum(), labels={\"index\": \"Programming Language\"}, title=\"Count of Programming Languages\")\n",
    "fig.update_layout(showlegend=False, xaxis={'categoryorder':'total descending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 4. let's get the ages from the original dataframe and join them to this new dataframe\n",
    "ages = df[\"Q1\"]\n",
    "language_dummies_with_age = language_dummies.join(ages).rename(columns={\"Q1\": \"Age\"})\n",
    "language_dummies_with_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_languages_by_age = language_dummies_with_age.groupby([\"Age\"]).sum()\n",
    "px.bar(programming_languages_by_age)\n",
    "# px.bar(programming_languages_by_age.T)\n",
    "programming_languages_by_age = programming_languages_by_age.reindex(\n",
    "    programming_languages_by_age.mean().sort_values().index, axis=1)\n",
    "programming_languages_by_age = programming_languages_by_age.iloc[:, ::-1]\n",
    "programming_languages_by_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_languages_by_age_row_norm = programming_languages_by_age.div(programming_languages_by_age.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# programming_languages_by_age.index\n",
    "programming_languages = programming_languages_by_age_row_norm.columns.tolist()\n",
    "fig = make_subplots(4, 3, subplot_titles=programming_languages_by_age_row_norm.index)\n",
    "for i, age_range in enumerate(programming_languages_by_age_row_norm.index):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=programming_languages, y=programming_languages_by_age_row_norm.iloc[i]),\n",
    "        row=row, col=col\n",
    "    )\n",
    "fig.update_layout(showlegend=False, height=1000, title=\"Percent of Known Programming Languages by Age\")\n",
    "fig.update_yaxes(tickformat=\"%\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like everyone likes Python! Younger people (who are most likely to be doing a bachelor's) have a relatively higher percentage of lower level languages like C, C++ and Java. This could be because they have to study these languages at university. As the data scientists specialise more in their career they seem to move away from these languages into more typical DSML related languages. The 60-69 group has a high level of R users relative to the other age groups, while it seems like most people over 70 don't know any programming languages.\n",
    "\n",
    "Perhaps more useful to us is what programming languages are popular against what job roles (Q5). Create a plot which demonstrates this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join the Job Titles question with the language_dummies dataframe\n",
    "language_dummies_with_job = language_dummies.join(df[\"Q5\"]).rename(columns={\"Q5\": \"Job Title\"})\n",
    "language_dummies_with_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group by Job Title, and aggregate the number, normalize each row, and sort the dataframe on the mean of the columns\n",
    "languages_job_title_grouped = language_dummies_with_job.groupby([\"Job Title\"]).sum()\n",
    "languages_job_title_grouped = languages_job_title_grouped.div(languages_job_title_grouped.sum(axis=1), axis=0)\n",
    "languages_job_title_grouped = languages_job_title_grouped.reindex(\n",
    "    languages_job_title_grouped.mean().sort_values().index, axis=1)\n",
    "languages_job_title_grouped = languages_job_title_grouped.iloc[:, ::-1]\n",
    "languages_job_title_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(languages_job_title_grouped, title=\"Heatmap of Programming Languages and Job Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the data!\n",
    "programming_languages = languages_job_title_grouped.columns.tolist()\n",
    "fig = make_subplots(4, 3, subplot_titles=languages_job_title_grouped.index)\n",
    "for i, role in enumerate(languages_job_title_grouped.index):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "#     numbers = langugages_job_title_grouped.iloc[i]\n",
    "#     as_percent = [number / sum(numbers) for number in numbers]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=programming_languages, y=languages_job_title_grouped.iloc[i]),\n",
    "        row=row, col=col\n",
    "    )\n",
    "fig.update_layout(showlegend=False, height=1000, title=\"Percent of Known Programming Languages Usage per Job Role\")\n",
    "fig.update_yaxes(tickformat=\"%\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "- Python is averagly the most popular language amongst all job roles\n",
    "- SQL is the most popular language amongst database engineers\n",
    "- MATLAB is relatively more popular amongst research scientists than other job roles\n",
    "- Statistician's prefer R over Python\n",
    "- Student's and oftware engineers prefer C++ over R\n",
    "\n",
    "Let's do one more plot - a heatmap of the framework that each job role likes to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the top 3 frameworks that each job role likes to use\n",
    "# 28p1 - 28p12\n",
    "## Create a dateframe which contains just the framework columns\n",
    "framework_cols = [\"Q28_Part_{}\".format(str(i)) for i in range(1, 13)]\n",
    "framework_df = df[framework_cols]\n",
    "framework_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a general function, `get_df_for_dummies()`, which takes in a dataframe, a column name prefix string and an upper range, and returns a dataframe populated with the range of columns based on the prefix string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_for_dummies(df, prefix_string, end_range, start_range=1):\n",
    "    ## HINT: use a range function and iteration to genereate the column names\n",
    "    dummies_cols = [prefix_string + str(i) for i in range(start_range, end_range)]\n",
    "    ## HINT: extract and return the column names from the dataframe\n",
    "    return df[dummies_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a column in this dataframe called 'frameworks used', and populate that column by comma separated frameworks\n",
    "framework_col = []\n",
    "for row in framework_df.itertuples(index=False):\n",
    "    frameworks_used = [framework for framework in row if isinstance(framework, str)]\n",
    "    framework_col.append(\",\".join(frameworks_used))\n",
    "    \n",
    "framework_df[\"frameworks_used\"] = framework_col\n",
    "framework_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace blank and None columns with \"None/NA\"\n",
    "values_to_update = {\"frameworks_used\": {\"\": \"None/NA\", \"None\": \"None/NA\"}}\n",
    "framework_df = framework_df.replace(values_to_update)\n",
    "framework_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a general function, `get_dummies()`, which takes in a dataframe and returns a column populated with comma separated values of each of the individual values over the dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies_col(df, sep=\",\"):\n",
    "\n",
    "    ## initialise an empty list to hold the column of strings which will be used to create a dummies dataframe\n",
    "    dummies_col = []\n",
    "    \n",
    "    ## iterate over each of the rows in the dataframe, in a manner where we can access the individual elements of the cells\n",
    "    ## get a list of the values of the cells over the row (e.g. a list of programming languages). Make sure you don't add nan's to your \n",
    "    ## join these as a comma separated string, and append it to the empty list for the column\n",
    "    for row in df.itertuples(index=False):\n",
    "        values = [item for item in row if isinstance(item, str)]\n",
    "        dummies_col.append(sep.join(values))\n",
    "        \n",
    "    ## create a new column in the dataframe called \"dummies\", which takes on the contents of dummies column\n",
    "    df[\"dummies\"] = dummies_col\n",
    "    \n",
    "    ## replace all the \"\" and \"None\"s from from the dataframe with \"None/NA\"\n",
    "    values_to_update = {\"dummies\": {\"\": \"None/NA\", \"None\": \"None/NA\"}}\n",
    "    df = df.replace(values_to_update)\n",
    "    \n",
    "    ## return the new dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dummies dataframe for the frameworks\n",
    "framework_dummies = framework_df['frameworks_used'].str.get_dummies(sep=',')\n",
    "framework_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a general function, `dummies_from_series()` which takes in a dataframe and a seperator argument, and returns dummies for the Series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummies_from_series(series, sep=\",\"):\n",
    "    ## return a dummies dataframe from the dataframe. \n",
    "    # Remember which column was used to assign the strings we want to create dummies over \n",
    "    return series[\"dummies\"].str.get_dummies(sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new dataframe which join this dataframe with job roles\n",
    "frameworks_for_job_role = framework_dummies.join(df[\"Q5\"]).rename(columns={\"Q5\": \"Job Title\"})\n",
    "frameworks_for_job_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group the dataframe by the job title, and aggregate over the programming languages\n",
    "frameworks_for_job_role_grouped = frameworks_for_job_role.groupby([\"Job Title\"]).sum()\n",
    "frameworks_for_job_role_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a general function, `group_dummies_by()` which takes in a dummies dataframe, and a Series, and returns the dummies grouped by and aggregated by the Series.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_dummies_by(dummies_df, series):\n",
    "    series_name = series.name\n",
    "    to_group = dummies_df.join(series)\n",
    "    grouped = to_group.groupby([series_name]).sum()\n",
    "    \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_df = get_df_for_dummies(df, \"Q28_Part_\", 13)\n",
    "framework_df = get_dummies_col(framework_df)\n",
    "framework_dummies = dummies_from_series(framework_df)\n",
    "frameworks_for_job_role_grouped = group_dummies_by(framework_dummies, df[\"Q5\"])\n",
    "frameworks_for_job_role_grouped = frameworks_for_job_role_grouped.div(\n",
    "    frameworks_for_job_role_grouped.sum(axis=1), axis=0)\n",
    "frameworks_for_job_role_grouped.index.rename(\"Job Role\", inplace=True)\n",
    "frameworks_for_job_role_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Produce a heatmap of the above dataframe!\n",
    "px.imshow(frameworks_for_job_role_grouped, title=\"Heatmap of preferred Frameworks per Job Role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "- None/NA may be slightly misleading. It doesn't always imply that their role has no need for frameworks because it also incorporates people who may not have answered the question because their tool of use wasn't provided as an option. For example, a statistician may beusing an R framework who's option isn't provided here.\n",
    "- Scikit-learn is the most popular listed tool\n",
    "- Statistician's seem to use random forest a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yearly Compensation\n",
    "\n",
    "Let's start considering our target variable now! We'll start basic and work our way up. Let's plot a histogram of salary earnings, and sort have this plot in order of salary ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, \"Q10\", labels={\"Q10\": \"Salary\"}, title=\"Count of salary ranges (Unsorted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can already see some interesting findings from this, but before I mention anything I want us to sort the x axis in order of numerical value. That is, the leftmost column is `$0-999` and the rightmost is `> $500,000`. \n",
    "\n",
    "Spend some time thinking and implementing how you would solve this problem. If after a few minutes you aren't able to think/come up with a solution, read and implement the spoiler below.\n",
    "<details>\n",
    "<summary><b>> Click here to reveal my solution</b></summary>\n",
    "<ul>\n",
    "<li>Create a new dataframe with just the salarys</li>\n",
    "<li>Get the set of salaries</li>\n",
    "<li>Create a mapping of the salary categories to an int, where the int is the first numerical part of the string (e.g. <code>{\"$0-999\": 0, \"100,000-124,999\": 100000}</code>). This part will require you to use the `.replace()` and `.split()` methods native to Python\n",
    "<li>Replace the salaries in the dataframe with the integer value</li>\n",
    "<li>Sort the dataframe in ascending numerical order</li>\n",
    "<li>Reverse the maping and replace the ints with their string variant</li>\n",
    "<li>Plot the dataframe, and replace the x labels with the salary strings</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = pd.DataFrame(df[\"Q10\"])\n",
    "salary_df.rename(columns={\"Q10\": \"Salary\"}, inplace=True)\n",
    "salary_set = set(salary_df[\"Salary\"])\n",
    "salary_string_int_dict = dict()\n",
    "\n",
    "for string_salary in salary_set:\n",
    "    \n",
    "    if isinstance(string_salary, float): continue\n",
    "        \n",
    "    salary = string_salary.replace(\"$\", \"\").replace(\"> \", \"\").replace(\",\", \"\")\n",
    "    salary = salary.split(\"-\")[0]\n",
    "    salary_string_int_dict[string_salary] = int(salary)\n",
    "\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "salary_df = salary_df.replace(values_to_update)\n",
    "salary_df = salary_df.sort_values(\"Salary\")\n",
    "\n",
    "salary_int_string_dict = {v:k for k,v in salary_string_int_dict.items()}\n",
    "values_to_update = {\"Salary\": salary_int_string_dict}\n",
    "salary_df = salary_df.replace(values_to_update)\n",
    "\n",
    "percent_na = np.round(100 * salary_df[\"Salary\"].isna().sum()/len(salary_df), 2)\n",
    "print(\"Percent of users who didn't answer the salary question:\", percent_na)\n",
    "px.histogram(salary_df, \"Salary\", title=\"Count of Salary ranges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 37% of the survey participants explicitly didn't answer this question. There seems to be an oddly large amount of people who are earning betweeen \\\\$0 and \\\\$999 a year. I suspect that this is so high because a lot of people who didn't want to answer the question (and didn't realise it was optional) ticked this box. We can also spot some other interesting facts - that is - we seem to have \"two\" peaks at vastly different salaries - one at 10,000 - 14,999 and the other at 100,000 - 124,999. Can you think why this could be? Also it looks like there are some very rich kagglers, with 83 of them earning over \\\\$500k a year.\n",
    "\n",
    "The top 3 most popular wages (par 0 - 999) seem to be 10,000 - 14,999, 100,000 - 124,999 and 30,000 - 39,999. Produce a choropleth plot of the median salary of the countries so we can better discern in what regions we can expect to earn what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Produce a choropleth plot of the median salary\n",
    "median_salaries_df = df[[\"Q3\", \"Q10\"]]\n",
    "median_salaries_df.rename(columns={\"Q3\": \"Country\", \"Q10\": \"Salary\"}, inplace=True)\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "median_salaries_df = median_salaries_df.replace(values_to_update)\n",
    "median_salaries_df = median_salaries_df.groupby([\"Country\"]).median()\n",
    "median_salaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = []\n",
    "for country in median_salaries_df.index:\n",
    "    country_code = pycountry.countries.search_fuzzy(country)[0] # Take the first element returned from the search\n",
    "    country_codes.append(country_code.alpha_3)\n",
    "\n",
    "median_salaries_df[\"Country Code\"] = country_codes\n",
    "median_salaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_series = median_salaries_df[\"Salary\"]\n",
    "values_to_update = {\"Salary\": salary_int_string_dict}\n",
    "median_salaries_df = median_salaries_df.replace(values_to_update)\n",
    "median_salaries_df[\"Salary Values\"] = salaries_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.choropleth(median_salaries_df, locations=\"Country Code\", hover_name=median_salaries_df.index, color=\"Salary Values\", hover_data=[\"Salary\"], title=\"Median Salaries by Country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the amount of participants from USA and India that we discerned earlier on, we can take these values to be more correct to the underlying data generation distribution than most other countries. Working under this assumption, it looks like in the US and Switzerland, it would be reasonable to be on 100,000+, whereas those in India would most likely be earning something in the 7,500 - 9,999 range. It seems Australia has some high paying jobs too. We'd expect the average salary in the UK to be higher than the 10,000 - 14,999 mark. What are some things we could investigate to identify why this salary value is lower than we expect it to be?\n",
    "\n",
    "What about salaries by gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_by_gender_df = df[[\"Q2\", \"Q10\"]]\n",
    "salaries_by_gender_df.rename(columns={\"Q2\": \"Gender\", \"Q10\": \"Salary\"}, inplace=True)\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "salaries_by_gender_df = salaries_by_gender_df.replace(values_to_update)\n",
    "salaries_by_gender_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(salaries_by_gender_df, \"Gender\", \"Salary\", labels={\"Salary\": \"Salary (Lower Bound)\"}, title=\"Boxplot of Salary per Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it looks like any of the categories are able to reach the highest salary bracket, it seems that females have the lowest median salary. Those who prefer to self describe seem to be more likely to have higher average salaries too.\n",
    "\n",
    "What are the best paying jobs? Let's plot the mean salary of each role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_job_df = df[[\"Q5\", \"Q10\"]]\n",
    "salaries_job_df.rename(columns={\"Q5\": \"Job Title\", \"Q10\": \"Salary\"}, inplace=True)\n",
    "\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "salaries_job_df = salaries_job_df.replace(values_to_update)\n",
    "salaries_job_df\n",
    "\n",
    "# salary_series = salaries_job_df[\"Salary\"]\n",
    "# salaries_job_df[\"Salary Bracket\"] = salary_series\n",
    "# salaries_job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mean_salaries = salaries_job_df.groupby([\"Job Title\"]).mean().reset_index().sort_values(by=\"Salary\", ascending=False)\n",
    "grouped_mean_salaries.dropna(inplace=True)\n",
    "px.bar(grouped_mean_salaries, \"Job Title\", \"Salary\", labels={\"Salary\": \"Mean Salary\"}, title=\"Mean Salary per Job Role\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could perhaps be a more informative way to represent these salaries? I'm thinking boxplot. Let's drop the NAs and \"0\" salaries and visualise this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_job_df.dropna(inplace=True)\n",
    "salaries_job_df = salaries_job_df[(salaries_job_df[\"Salary\"] != 0)]\n",
    "fig = px.box(salaries_job_df, \"Job Title\", \"Salary\", labels={\"Salary\": \"Salary (Lower Bound of Bracket)\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "- The survey seems to indicate that all job roles have the potential to gain a job at \\\\$500k+ apart from that of a Database Engineer\n",
    "- Globally, Software Engineers and Data Analysts have the two lowest paying average salaries - with a software engineer having a lower median salary than a data analyst, but a higher mean (as seen from the first chart)\n",
    "- Product/Project Management and Data Science seem to be the most lucrative job roles to be in - with the former slightly taking the lead\n",
    "\n",
    "I wonder the percent of applicable Data Scientists who earn above \\\\$500k vs Project Managers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_scientists_above_500 = len(salaries_job_df[(salaries_job_df[\"Job Title\"] == \"Data Scientist\") & (salaries_job_df[\"Salary\"] == 500000)])\n",
    "num_project_managers_above_500 = len(salaries_job_df[(salaries_job_df[\"Job Title\"] == \"Product/Project Manager\") & (salaries_job_df[\"Salary\"] == 500000)])\n",
    "\n",
    "percent_ds_above_500 = 100 * num_data_scientists_above_500/len(salaries_job_df)\n",
    "percent_pm_above_500 = 100 * num_project_managers_above_500/len(salaries_job_df)\n",
    "\n",
    "print(\"The percent of Data Scientists who earn above $500,000: {}%\".format(np.round(percent_ds_above_500, 2)))\n",
    "print(\"The percent of Project Managers who earn above $500,000: {}%\".format(np.round(percent_pm_above_500, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much effect years of programming (Q11) have against salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_experience_salary_df = df[[\"Q10\", \"Q15\"]]\n",
    "programming_experience_salary_df.rename(columns={\"Q10\": \"Salary\", \"Q15\": \"Programming Experience\"}, inplace=True)\n",
    "values_to_update = {\"Salary\": salary_string_int_dict}\n",
    "programming_experience_salary_df = programming_experience_salary_df.replace(values_to_update)\n",
    "programming_experience_salary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_array = [\"I have never written code\", \"< 1 years\", \"1-2 years\", \"3-5 years\", \"5-10 years\", \"10-20 years\", \"20+ years\"]\n",
    "# fig = px.scatter(programming_experience_salary_df, \"Programming Experience\", \"Salary\", title=\"Density of Programming Experience vs Salary\")\n",
    "fig = px.scatter(programming_experience_salary_df, \"Programming Experience\", \"Salary\", facet_col=df[\"Q2\"],title=\"Density of Programming Experience vs Salary\")\n",
    "fig.update_traces(marker=dict(\n",
    "            opacity=0.05,\n",
    "            size=20,\n",
    "            line=dict(\n",
    "                color='MediumPurple',\n",
    "                width=0.5\n",
    "            )))\n",
    "fig.update_layout(xaxis={'categoryorder':'array', 'categoryarray':category_array})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "- There are people earning \\\\$500k+ who haven't ever written code. Same with people who have under a years worth of programming experience.\n",
    "- There is a trend however with more experienced programmers earning a higher salary across all levels of salary brackets\n",
    "- There aren't that many experienced females (i.e. more than 10-20 years experience)\n",
    "\n",
    "Now we're going to plot another Sankey Diagram - this time tracking how Gender, Age, Degree, Role and Country all have an affect on your Salary. To achieve this, generalise the code I wrote earlier on for the Sankey diagram into a function, `get_sankey_data`, which takes a reindexed dataframe as argument and returns the node_labels and link dictionary. Subsequently, use these items to plot a Sankey diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function-ize the sankey diagram code I wrote earlier\n",
    "def get_sankey_data(reindexed_df):\n",
    "    col_names = reindexed_df.columns.tolist()\n",
    "    node_labels = []\n",
    "    num_categorical_vals_per_col = []\n",
    "    \n",
    "    for col in col_names:\n",
    "        uniques = reindexed_df[col].unique().tolist()\n",
    "        node_labels.extend(uniques)\n",
    "        num_categorical_vals_per_col.append(len(uniques))\n",
    "\n",
    "\n",
    "    source = []\n",
    "    target = []\n",
    "    value = []\n",
    "    for i, num_categories in enumerate(num_categorical_vals_per_col):\n",
    "\n",
    "        if i == len(num_categorical_vals_per_col)-1:\n",
    "            break\n",
    "\n",
    "        # index allows us to refer to the categories by index from the `node_labels` list\n",
    "        start_index = sum(num_categorical_vals_per_col[:i])\n",
    "        start_index_next = sum(num_categorical_vals_per_col[:i+1])\n",
    "        end_index_next = sum(num_categorical_vals_per_col[:i+2])\n",
    "\n",
    "\n",
    "        # i can also give us the category column to refer to\n",
    "        col_name = col_names[i]\n",
    "        next_col_name = col_names[i+1]\n",
    "\n",
    "        grouped_df = reindexed_df.groupby([col_name, next_col_name]).size()\n",
    "\n",
    "        for source_i in range(start_index, start_index_next):\n",
    "            for target_i in range(start_index_next, end_index_next):\n",
    "                source.append(source_i)\n",
    "                target.append(target_i)\n",
    "                source_label = node_labels[source_i]\n",
    "                target_label = node_labels[target_i]\n",
    "                # if the index doesn't exist in the grouped_df, then the value is 0\n",
    "                try:\n",
    "                    value.append(grouped_df[source_label][target_label])\n",
    "                except:\n",
    "                    value.append(0)\n",
    "\n",
    "#                 random_color = list(np.random.randint(256, size=3)) + [random.random()]\n",
    "#                 random_color_string = ','.join(map(str, random_color))\n",
    "#                 colors.append('rgba({})'.format(random_color_string))\n",
    "\n",
    "    link = dict(source=source, target=target, value=value)\n",
    "    return node_labels, link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new dataframe with the relevant variables we want to plot our Sankey with, re-index it, and pass it to the get_sankey_data function\n",
    "salaries_sankey_df = df[[\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\", \"Q10\"]]\n",
    "salaries_sankey_df.rename(columns={\"Q1\": \"Age\", \"Q2\": \"Gender\", \"Q3\": \"Country\", \"Q4\": \"Education\", \"Q5\": \"Role\", \"Q10\": \"Salary\"}, inplace=True)\n",
    "salaries_sankey_df = salaries_sankey_df.reindex([\"Gender\", \"Age\", \"Education\", \"Role\", \"Country\", \"Salary\"], axis=1)\n",
    "node_labels, link = get_sankey_data(salaries_sankey_df)\n",
    "node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = node_labels,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = link)])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram (Gender, Age, Education, Role, Country, Salary)\", font_size=10, height=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above diagram is really messy... Let's clean it up as follows:\n",
    "- Group countries by continent (again we'll leave India and USA as is)\n",
    "- Create wider bins for salary (maybe 5 bins?), dropping \\\\$0-999\n",
    "- Add in coding experience as a column in the above plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace countries with continents\n",
    "values_to_update = {\"Country\": countries_continent_dict}\n",
    "salaries_sankey_df = salaries_sankey_df.replace(values_to_update)\n",
    "salaries_sankey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows with $0-999 and create wider bins for salary\n",
    "set(salaries_sankey_df[\"Salary\"])\n",
    "salaries_sankey_df = salaries_sankey_df[salaries_sankey_df[\"Salary\"] != \"$0-999\"]\n",
    "under_30000, under_80000, under_150000, under_300000, under_500000 = \"0 - 29,999\", \"30,000 - 79,999\", \"80,000 - 149,999\", \"150,000 - 299,999\", \"300,000 - 500,000\" \n",
    "salary_wider_bins_dict = dict()\n",
    "for salary_string in set(salaries_sankey_df[\"Salary\"]):\n",
    "    \n",
    "    if salary_string == \"> $500,000\" or isinstance(salary_string, float):\n",
    "        continue\n",
    "    \n",
    "    salary_upper_bound = salary_string.split(\"-\")[-1]\n",
    "    salary_upper_bound = salary_upper_bound.replace(\",\", \"\")\n",
    "    salary_upper_bound = int(salary_upper_bound)\n",
    "    \n",
    "    if salary_upper_bound < 30000:\n",
    "        salary_wider_bins_dict[salary_string] = under_30000\n",
    "    elif salary_upper_bound < 80000:\n",
    "        salary_wider_bins_dict[salary_string] = under_80000\n",
    "    elif salary_upper_bound < 150000:\n",
    "        salary_wider_bins_dict[salary_string] = under_150000\n",
    "    elif salary_upper_bound < 300000:\n",
    "        salary_wider_bins_dict[salary_string] = under_300000\n",
    "    elif salary_upper_bound < 500000:\n",
    "        salary_wider_bins_dict[salary_string] = under_500000\n",
    "\n",
    "values_to_update = {\"Salary\": salary_wider_bins_dict}\n",
    "salaries_sankey_df = salaries_sankey_df.replace(values_to_update)\n",
    "salaries_sankey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column in salaries_sankey_df which is the programming experience length\n",
    "salaries_sankey_df[\"Programming Experience\"] = df[\"Q15\"]\n",
    "salaries_sankey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Reindex and plot the Sankey diagram!\n",
    "# Drop the Age and Gender columns and reindex the dataframe as:\n",
    "# Role, Programming Experience, Region, Education, Salary\n",
    "salaries_sankey_df = salaries_sankey_df.rename(columns={\"Country\": \"Region\"})\n",
    "salaries_sankey_df = salaries_sankey_df.reindex([\"Role\", \"Programming Experience\", \"Region\", \"Education\", \"Salary\"], axis=1)\n",
    "node_labels, link = get_sankey_data(salaries_sankey_df)\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = node_labels,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = link)])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram (Role, Programming Experience, Region, Education, Salary)\", font_size=10, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information and the paths to the different salary brackets becomes a lot clearer to interpret now. Some interesting findings that I've found from this graph is that there are some software engineers and data scientists who have never written code before - I'm unsure how this could be the case but they are small in number so we could chalk it up to anomalies. The majority of people earning over 150,000 seem to hold either master's or doctoral degrees. Playing around with the order of indexing can quickly allow you to draw insights from different combinations of columns (e.g. by putting Education before Job Role, we can see what roles people are likely to go into based on their education).\n",
    "\n",
    "One last plot! Question 9 asks about the skills and responsibilities that their current job entails. Produce subplots of these skills, aggregated, for each of the salary brackets. Use the 6 salary brackets we defined previously. Ensure that the columns of the skills dataframe you create are intact, and demonstrate your plot/and any other python output in an interpretable manner - whether this be the strings on the axis or the ordering of the facets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_df = get_df_for_dummies(df, \"Q9_Part_\", 9)\n",
    "skills_df = get_dummies_col(skills_df, sep=\"::\")\n",
    "skills_dummies = dummies_from_series(skills_df, sep=\"::\")\n",
    "skills_grouped = group_dummies_by(skills_dummies, salaries_sankey_df[\"Salary\"])\n",
    "skills_grouped = skills_grouped.reindex([\"0 - 29,999\", \"30,000 - 79,999\", \"80,000 - 149,999\", \"150,000 - 299,999\", \"300,000-500,000\", \"> $500,000\"])\n",
    "skills_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_df_columns = skills_grouped.columns\n",
    "skills_df_columns_mapping = {col: i for i, col in enumerate(skills_df_columns)}\n",
    "skills_grouped = skills_grouped.rename(columns=skills_df_columns_mapping)\n",
    "[print(i, \"\\t\", col) for col, i in skills_df_columns_mapping.items()]\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Produce plots! The x axis should be top 4 programming languages, and the y axis the count of them.\n",
    "fig = make_subplots(2, 3, subplot_titles=skills_grouped.index)\n",
    "for i, role in enumerate(skills_grouped.index):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    skills_values = skills_grouped.iloc[i]\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=skills_values.index, y=skills_values),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    fig.update_xaxes(type=\"category\")\n",
    "fig.update_layout(showlegend=False, height=800, title=\"Skills/Responsibilities per Salary Bracket\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "- Across all salary brackets, 0, \"Analyze and understand data to influence product or business decisions\" is present.\n",
    "- Apart from the lowest and highest bracket, it looks like a lot of jobs involve 3, builing prototypes which explores applying machine learning to new areas.\n",
    "- The top two salary brackets have a higher relative frequency of answer 4 - Do research that advances the state of the art of machine learning.\n",
    "- The last three brackets seem to have a higher proportion of employess who have option 5 (Experimentation and iteration to improve existing ML models) as a responsibility.\n",
    "- Option 7, None/NA, seems to be more prevalent in the bottom two brackets and the uppermost bracket. I suspect the reason for this is that none of the job descriptions apply to these individuals, whereas the other three brackets have jobs responsibilities focused more on data science and machine learning. \n",
    "\n",
    "What kind of plot may have provided this information in a cleaner and consise manner?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
